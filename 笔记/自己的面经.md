# --JVM

[Java虚拟机笔记]: F:\java书和视频\笔记\Java虚拟机.md

## 常用指令（JVM和Java）

jps：显示当前正在执行的任务的进程号

jstack：堆栈跟踪工具，打印出给定的java进程ID的堆栈信息，生成java虚拟机当前时刻的线程快照。线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的原因主要目的是定位线程出现长时间停顿的原因，**如线程间死锁、死循环、请求外部资源导致的长时间等待**等。

javap -c：将字节码反汇编

jvisualvm：查看运行时数据区的使用情况，虚拟机实时监控

# ==-- IO==

[JavaNIO]: F:\java书和视频\笔记\NIO.md

# ==--Java多线程==

[并发编程笔记]: F:\java书和视频\笔记\Java并发编程.md

## 一、Java线程池的原理和使用

### 使用线程池的好处

1. **降低资源消耗**

   可以重复利用已创建的线程降低线程创建和销毁造成的消耗

2. **提高响应速度**

   当任务到达时，任务可以不需要等到线程创建就能立即执行

3. **提高线程的可管理性**

   线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控

https://blog.csdn.net/fuyuwei2015/article/details/72758179

Java并发编程的艺术

ThreadPoolExecutor

## 二、对象头中的Mark Word

Mark Word记录了对象和锁有关的信息，当这个对象被synchronized关键字当成同步锁时，围绕这个锁的一系列操作都和Mark Word有关。其在32位JVM中是这样存储的

![image-20200714132435335](F:\java书和视频\笔记\images\面经\4.png)

JVM一般是这样使用锁和Mark Word的：

1. 当没有被当成锁时，这就是一个普通的对象，Mark Word记录对象的HashCode，锁标志位是01，是否偏向锁那一位是0。

2. 当对象被当做同步锁并有一个线程A抢到了锁时，锁标志位还是01，但是否偏向锁那一位改成1，前23 bits记录抢到锁的线程id，表示进入偏向锁状态。

3. 当线程A再次试图来获得锁时，JVM发现同步锁对象的标志位是01，是否偏向锁是1，也就是偏向状态，Mark Word中记录的线程id就是线程A自己的id，表示线程A已经获得了这个偏向锁，可以执行同步锁的代码。

4. 当线程B试图获得这个锁时，JVM发现同步锁处于偏向状态，但是Mark Word中的线程id记录的不是B，那么线程B会先用CAS操作试图获得锁，这里的获得锁操作是有可能成功的，因为线程A一般不会自动释放偏向锁。如果抢锁成功，就把Mark Word里的线程id改为线程B的id，代表线程B获得了这个偏向锁，可以执行同步锁代码。如果抢锁失败，则继续执行步骤5。

5. **偏向锁状态抢锁失败，代表当前锁有一定的竞争**，**偏向锁将升级为轻量级锁**。JVM会在当前线程的线程栈中开辟一块单独的空间，里面保存指向对象锁Mark Word的指针，同时在对象锁Mark Word中保存指向这片空间的指针。上述两个保存操作都是CAS操作，如果保存成功，代表线程抢到了同步锁，就把Mark Word中的锁标志位改成00，可以执行同步锁代码。如果保存失败，表示抢锁失败，竞争太激烈，继续执行步骤6。

6. 轻量级锁抢锁失败，JVM会使用自旋锁，自旋锁不是一个锁状态，只是代表不断的重试，尝试抢锁。从JDK1.7开始，自旋锁默认启用，自旋次数由JVM决定。如果抢锁成功则执行同步锁代码，如果失败则继续执行步骤7。

7. 自旋锁重试之后如果抢锁依然失败，同步锁会升级至重量级锁，锁标志位改为10。在这个状态下，未抢到锁的线程都会被阻塞。
   

## 三、悲观锁与乐观锁

悲观锁和乐观锁是锁的一种宏观分类，并不是指某个特定的锁，而是在并发情况下的两种不同策略

### 悲观锁

每次要获取数据都会上锁，其他线程想获取相同数据的时候就只能阻塞，直到锁被释放。关系型数据库里用到了很多这种锁机制，比如行锁、表锁、读锁、写锁等。**Java中`synchronized`和`reentrantLock`等独占锁就是悲观锁思想的实现**

### 乐观锁

每次获取数据的时候不会上锁，但是在更新的时候会判断一下在此期间其他线程有没有更新此数据，可以使用**版本号机制和CAS算法**来实现。乐观锁适用于多读的应用类型，可以提高吞吐量。**Java中`java.util.concurrent.atomic`包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的**

### 乐观锁常见的两种实现方式

1. 版本号机制

    一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读数据的同时也会读取version值，在提交更新时，若刚才读取到的version值与当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功（也可能会被驳回更新操作）。

2. CAS算法（乐观锁的基础）

    Compare and Swap，是一种无锁算法，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫**非阻塞同步**。CAS涉及到三个操作数

    - 需要读写的内存值V
    - 进行比较的值A
    - 拟写入的新值B

    当且仅当V的值等于A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作。一般情况下是一个自旋操作，即不断地重试

## 四、自旋锁原理及Java自旋锁

### 一、自旋锁的概念

基本作用是用于线程（进程）之间的同步。与普通锁不同的是，线程A在获取普通锁后，线程B要想获取相同的锁，那么线程B将会挂起（阻塞）；如果在两个线程竞争不是特别激烈的情况下（锁拥有者持有锁的时间短），线程的挂起与唤醒、线程上下文的切换会造成更多的资源消耗，这时线程B可以不放弃CPU的时间片，而是在“原地”忙等，直到锁的持有者释放了该锁，这就是自旋锁的原理。

### 二、自旋锁可能存在的问题

1. 如果锁的持有者一直不释放锁，那么等待线程会一直等待，这会长时间占用CPU时间片导致CPU资源的浪费，因此可以设定一个时间，当锁持有者超过这个时间不释放锁，等待者会放弃CPU时间片阻塞
2. 死锁：递归程序使用自旋锁时可能会造成死锁。因此，递归程序绝不能再持有自旋锁时调用它自己，也不能在递归调用时试图获得相同的自旋锁

### 三、使用CAS实现自旋锁

```java
public class SpinLock {

  private AtomicReference<Thread> sign =new AtomicReference<>();

  public void lock(){
    Thread current = Thread.currentThread();
    while(!sign .compareAndSet(null, current)){
    }
  }

  public void unlock (){
    Thread current = Thread.currentThread();
    sign.compareAndSet(current, null);
  }
}
```

lock函数将owner设置为当前线程，并且预测原来的值为空。unlock函数将owner设置为null，并且预测值为当前线程。

当有第二个线程调用lock操作时由于owner值不为空，导致循环一直执行，直至第一个线程调用unlock函数将owner设置为null，第二个线程才能进入临界区。

由于自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。

### 四、自旋锁的其他种类

有三种常见的自旋锁形式：TicketLock，CLHLock，MCSLock

## 五、锁升级：偏向锁->轻量级锁->重量级锁

这是对Synchronized的一种优化。这里的轻量级锁是一种自旋锁。升级过程如下：

初次执行到synchronized代码块的时候，锁对象变成**偏向锁**（通过CAS修改对象头里的锁标志位），也就是偏向于第一个获得它的线程。执行完同步代码块后，**线程并不会主动释放偏向锁**。当第二次到达同步代码块时，线程会判断此时持有锁的线程是否就是自己（持有锁的线程ID也在对象头里），如果是则正常往下执行。由于之前没有释放锁，这里也就不需要重新加锁。如果始终只有一个线程在执行任务，那么偏向锁几乎没有开销。

当有第二个线程要来获取锁时，第二个线程会首先检查锁对象中记录的线程是否还存活，如果这个对象已经不再存活了，那么锁对象被置为无锁状态，其他线程可以竞争使用CAS来将锁对象中的记录赋值为它们自己的ID。如果存在的话，那么就会去栈帧中检查已获取锁的线程是否还需要继续持有这个锁，如果还需要继续持有这个锁，那么就会暂停当前线程，然后将锁对象中的对象头中记录的锁状态改为轻量级锁（使用CAS），然后再恢复之前的线程。其他要竞争锁的线程就在轻量级锁状态下继续锁竞争，没有抢到锁的线程将自旋，即不停地循环判断锁是否能够被成功获取。**获取锁的操作，其实就是通过CAS修改对象头里的锁标志位**。先比较当前锁标志是否为“释放”，如果是则将其设置为“锁定”，比较并设置是原子性发生的。这就算抢到锁了，然后线程将当前锁的持有者信息修改为自己。

长时间的自旋操作非常消耗资源，占用CPU时间片，这种现象叫作**忙等**。如果锁竞争不是很激烈，那么使用轻量级锁是没有什么问题的。

如果锁竞争情况十分严重，某个达到**线程最大自旋次数**的线程，会将轻量级锁升级为重量级锁（依然是CAS修改锁标志位，但不修改持有锁的线程ID）。当后续线程尝试持有锁时，发现被占用的锁是重量级锁，则直接将自己挂起

**注意：**锁只能升级，不能降级。偏向锁升级为轻量级锁过后，不能再降级为偏向锁，目的是为了提高获得锁和释放锁的效率。

==偏向锁的一个特性是，持有锁的线程在执行完同步代码块时不会释放锁。那么当第二个线程执行到这个synchronized代码块时是否一定会发生锁竞争然后升级为轻量级锁呢？==
线程A第一次执行完同步代码块后，当线程B尝试获取锁的时候，发现是偏向锁，会判断线程A是否仍然存活。**如果线程A仍然存活，**将线程**A暂停**，此时偏向锁升级为轻量级锁，之后线程**A继续执行**，线程**B自旋**。但是**如果判断结果是线程A不存在了**，则线程B持有此偏向锁，锁不升级。	

**偏向锁的取消**

偏向锁是默认开启的，而且开始时间一般比应用程序启动慢几秒，可以使用`-XX:BiasedLockingStartUpDelay=0`来将延迟设置为0

如果不想使用偏向锁，可以通过`-XX:UseBiasedLocking=false`来设置

**轻量级锁的加锁**

线程在执行同步块之前，JVM会现在当前线程的栈帧中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。



## 六、AbstractQueuedSynchronizer

队列同步器AbstractQueuedSynchronizer是用来构建锁或者其他同步组件的基础框架，它使用一个**int成员变量表示同步状态**，通过**内置的FIFO队列**来完成资源获取线程的排队工作。

同步器的主要使用方式是**继承**，子类通过继承同步器并实现它的抽象方法来管理同步状态，在抽象方法的实现过程中免不了要对同步状态进行更改，这时就需要使用同步器提供的3个方法（`getState()`、`setState(int newState)`和`compareAndSetState(int expect,int update)）`来进行操作，因为它们能够保证状态的改变是安全的。

子类推荐**被定义为自定义同步组件的静态内部类**，同步器自身没有实现任何同步接口，它仅仅是**定义了若干同步状态获取和释放的方法来供自定义同步组件使用**，同步器既可以支持独占式地获取同步状态，也可以支持共享式地获取同步状态，这样就可以方便实现不同类型的同步组件（`ReentrantLock`、`ReentrantReadWriteLock`和`CountDownLatch`等）。

### 1、AQS的接口与示例

AQS的设计是基于==**模板方法模式**==的，也就是说，使用者需要继承AQS并重写指定的方法，随后将AQS组合在自定义同步组件的实现中，并调用AQS提供的模板方法，而**这些模板方法将会调用使用者重写的方法**

重写同步器指定的方法时，需要使用同步器提供的如下3个方法来访问或修改同步状态。

* getState()：获取当前同步状态
* setState(int new State)：设置当前同步状态
* compareAndSetState(int expect, int update)：使用CAS设置当前状态，该方法能够保证状态设置的原子性

同步器可重写的方法：

![image-20200418143052832](F:\Java书和视频\笔记\images\并发编程\18.png)

实现自定义同步组件时，将会调用同步器提供的模板方法，这些模板方法与描述如下：

![image-20200418143524592](F:\Java书和视频\笔记\images\并发编程\19.png)

AQS提供的模板方法基本上分为3类：独占式获取与释放同步状态、共享式获取与释放同步状态和查询同步队列中的等待线程情况。自定义同步组件将使用AQS提供的模板方法来实现自己的同步语义。

```java
public class Mutex implements Lock {
    
    //静态内部类，自定义同步器
    private static class Sync extends AbstractQueuedSynchronizer {
        //是否处于独占状态

        @Override
        protected boolean isHeldExclusively() {
            return getState() == -1;
        }
        
        //当状态为0的时候获取锁
        @Override
        protected boolean tryAcquire(int arg) {
            if (compareAndSetState(0, 1)) {
                setExclusiveOwnerThread(Thread.currentThread());
                return true;
            }
            return false;
        }
        
        //释放锁，将状态设置为0
        @Override
        protected boolean tryRelease(int arg) {
            if (getState() == 0)
                throw new IllegalMonitorStateException();
            setExclusiveOwnerThread(null);
            setState(0);
            return true;
        }

        //返回一个Condition，每个Condition都包含一个condition队列
        Condition newCondition() { return new ConditionObject();}
    }
    //将操作代理到Sync上即可
    private final Sync sync = new Sync();
    @Override
    public void lock() {
        sync.acquire(1);
    }

    @Override
    public void lockInterruptibly() throws InterruptedException {
        sync.acquireInterruptibly(1);
    }

    @Override
    public boolean tryLock() {
        return sync.tryAcquire(1);
    }

    @Override
    public boolean tryLock(long time, TimeUnit unit) throws InterruptedException {
        return sync.tryAcquireNanos(1, unit.toNanos(time));
    }

    @Override
    public void unlock() {
        sync.release(1);
    }

    @Override
    public Condition newCondition() {
        return sync.newCondition();
    }
}

```

上述示例中，独占锁Mutex是一个自定义同步组件，它在同一时刻只允许一个线程占有锁。Mutex中定义了一个静态内部类，该内部类继承了同步器并实现了独占式获取和释放同步状态。在`tryAcquire(int acquires)`方法中，如果经过CAS设置成功（同步状态设置为1），则代表获取了同步状态，而在`tryRelease(int releases)`方法中只是将同步状态重置为0。用户使用Mutex时并不会直接和内部同步器的实现打交道，而是调用Mutex提供的方法，在Mutex的实现中，以获取锁的`lock()`方法为例，只需要在方法实现中调用同步器的模板方法`acquire(int args)`即可，当前线程调用该方法获取同步状态失败后会被加入到同步队列中等待，这样就大大降低了实现一个可靠自定义同步组件的门槛。

### 2、队列同步器的实现分析

1. **同步队列**

    同步器依赖内部的同步队列（一个FIFO双向队列）来完成同步状态的管理，**当前线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成为一个节点（Node）并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点中的线程唤醒，使其再次尝试获取同步状态**。

    同步队列中的节点（Node）用来保存获取同步状态失败的线程引用、等待状态以及前驱和后继节点

    ![image-20200418145433615](F:\Java书和视频\笔记\images\并发编程\20.png)

    节点是构成同步队列的基础，同步器拥有首节点（head）和尾节点（tail），没有成功获取同步状态的线程将会成为节点加入该队列的尾部，同步队列的基本结构如图：

    ![image-20200418145558381](F:\Java书和视频\笔记\images\并发编程\21.png)

    同步器包含了两个节点类型的引用，一个指向头节点，而另一个指向尾节点。试想一下，当一个线程成功地获取了同步状态（或者锁），其他线程将无法获取到同步状态，转而被构造成为节点并加入到同步队列中，而这个加入队列的过程必须要保证线程安全，因此同步器提供了一个基于CAS的设置尾节点的方法：compareAndSetTail(Node expect,Nodeupdate)，它需要传递当前线程“认为”的尾节点和当前节点，只有设置成功后，当前节点才正式与之前的尾节点建立关联。

    ==**首节点是获取同步状态成功的节点**==，**首节点的线程在释放同步状态时，将会唤醒后继节点，而后继节点将会在获取同步状态成功时将自己设置为首节点**

2. **独占式同步状态获取与释放**

    调用同步器的acquire(int arg)方法可以获取同步状态，该方法**对中断不敏感**，也就是由于线程获取同步状态失败后进入同步队列中，后续对线程进行中断操作，线程不会从同步队列中移出，该方法代码如下：

    ```java
    public final void acquire(int arg) {//这是一个模板方法，调用了我们重写的方法
        if (!tryAcquire(arg) && //tryAcquire方法是需要我们重写的方法
            //如果同步状态获取失败则构造节点（独占式Node.EXCLUSIVE，同一时刻只能有一个线程成功获取同步状态），通过addWaiter方法将节点加入到同步队列尾部，调用acquireQueued方法使得该结点以死循环的方式获取同步状态
            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))
            selfInterrupt();
    }
    这段代码主要完成了同步状态获取，如果获取成功则直接返回，如果没有获取成功则需要构造节点，加入同步队列，以及在同步队列中自旋等待的相关工作。
    ```

    在acquireQueued方法的实现中，**只有前驱节点是头节点的节点才能够尝试获取同步状态**，因为：

    1. 头节点是成功获取到同步状态的节点，而头节点的线程释放了同步状态之后，将会唤醒其后继节点，后继节点的线程被唤醒后需要检查自己的前驱节点是否是头节点
    2. 维护同步队列的FIFO原则。

    acquire方法的调用流程图如下：

    ![image-20200418151225751](F:\Java书和视频\笔记\images\并发编程\22.png)

    **总结**

    在获取同步状态时，同步器维护一个同步队列，获取状态失败的线程都会被加入到队列中并在队列中进行自旋；移除队列（或停止自旋）的条件是前驱节点为头节点且成功获取到同步状态。在释放同步状态时，同步器调用tryRelease()方法释放同步状态，然后唤醒头节点的后继节点

3. **共享式同步状态获取与释放**

    通过调用同步器的acquireShared(int arg)方法可以共享式地获取同步状态，该方法代码如下：

    ```java
    public final void acquireShared(int arg) {
        if (tryAcquireShared(arg) < 0)
            doAcquireShared(arg);
    } 
    private void doAcquireShared(int arg) {
        final Node node = addWaiter(Node.SHARED);
        boolean failed = true;
        try {
            boolean interrupted = false;
            for (;;) {
                final Node p = node.predecessor();
                if (p == head) {
                    int r = tryAcquireShared(arg);
                    if (r >= 0) {
                        setHeadAndPropagate(node, r);
                        p.next = null;
                        if (interrupted)
                            selfInterrupt();
                        failed = false;
                        return;
                    }
                }
                if (shouldParkAfterFailedAcquire(p, node) &&
                    parkAndCheckInterrupt())
                    interrupted = true;
            }
        } finally {
            if (failed)
                cancelAcquire(node);
        }
    }
    在acquireShared(int arg)方法中，同步器调用tryAcquireShared(int arg)方法尝试获取同步状态，tryAcquireShared(int arg)方法返回值为int类型，当返回值大于等于0时，表示能够获取到同步状态。因此，在共享式获取的自旋过程中，成功获取到同步状态并退出自旋的条件就是tryAcquireShared(int arg)方法返回值大于等于0。可以看到，在doAcquireShared(int arg)方法的自旋过程中，如果当前节点的前驱为头节点时，尝试获取同步状态，如果返回值大于等于0，表示该次获取同步状态成功并从自旋过程中退出。
    ```

4. **重入锁的实现**

    ReentrantLock是通过组合自定义同步器来实现锁的获取与释放，以非公平性（默认的）实现为例，获取同步状态的代码如下：

    ```java
    final boolean nonfairTryAcquire(int acquires) {
        final Thread current = Thread.currentThread();
        int c = getState();
        if (c == 0) {
            if (compareAndSetState(0, acquires)) {
                setExclusiveOwnerThread(current);
                return true;
            }
        } else if (current == getExclusiveOwnerThread()) {
            int nextc = c + acquires;
            if (nextc < 0)
                throw new Error("Maximum lock count exceeded");
            setState(nextc);
            return true;
        }
        return false;
    }
    ```

    该方法增加了再次获取同步状态的处理逻辑：**通过判断当前线程是否为获取锁的线程来决定获取操作是否成功，如果是获取锁的线程再次请求，则将同步状态值进行增加并返回true，表示获取同步状态成功。**

    成功获取锁的线程再次获取锁，只是增加了同步状态值，这也就要求ReentrantLock在释放同步状态时减少同步状态值，该方法的代码如下：

    ```java
    protected final boolean tryRelease(int releases) {
        int c = getState() - releases;
        if (Thread.currentThread() != getExclusiveOwnerThread())
            throw new IllegalMonitorStateException();
        boolean free = false;
        if (c == 0) {
            free = true;
            setExclusiveOwnerThread(null);
        }
        setState(c);
        return free;
    }
    ```

    如果该锁被获取了n次，那么前(n-1)次tryRelease(int releases)方法必须返回false，而只有同步状态完全释放了，才能返回true。可以看到，该方法将同步状态是否为0作为最终释放的条件，当同步状态为0时，将占有线程设置为null，并返回true，表示释放成功。

5. **公平锁的实现**

    对于非公平锁，只要CAS设置同步状态成功，则表示当前线程获取了锁，而公平锁则不同，如代码：

    ```java
    protected final boolean tryAcquire(int acquires) {
        final Thread current = Thread.currentThread();
        int c = getState();
        if (c == 0) {
            if (!hasQueuedPredecessors() && compareAndSetState(0, acquires)) {
                setExclusiveOwnerThread(current);
                return true;
            }
        } else if (current == getExclusiveOwnerThread()) {
            int nextc = c + acquires;
            if (nextc < 0)
                throw new Error("Maximum lock count exceeded");
            setState(nextc);
            return true;
        }
        return false;
    }
    ```

    该方法与1比较，唯一不同的位置为判断条件多了`hasQueuedPredecessors()`方法，即**加入了同步队列中当前节点是否有前驱节点的判断，如果该方法返回`true`，则表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并释放锁之后才能继续获取锁。**

    **公平锁和非公平锁的区别**

    公平性锁保证了锁的获取按照FIFO原则，而代价是进行大量的线程切换。非公平性锁虽然可能造成线程“饥饿”，但极少的线程切换，保证了其更大的吞吐量。

## 七、Synchronized

Synchronized是Java中的关键字，利用锁来实现同步。

它可以用于方法上，也可以用于代码块：

- 对于普通的同步方法，锁是当前实例对象
- 对于静态同步方法，锁是当前类的Class对象
- 对于同步方法块，锁是Synchronized括号里配置的对象

在底层是通过monitorenter和monitorexit指令来实现的。monitorenter在代码编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束和异常处，JVM要保证每一个monitorenter都有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，并且一个monitor被持有后，它将处于锁定状态（对象头里添加相应的标识）。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。

## 八、死锁产生的必要条件以及如何解除死锁

- 死锁产生的必要条件

    1. 互斥条件	

        一个资源每次只能被一个进程使用

    2. 请求与保持条件

        一个进程因请求资源而阻塞时，对已获得的资源保持不放

    3. 不剥夺条件

        进程已获得的资源，在未使用之前，不能被其他进程剥夺

    4. 循环等待条件

        若干进程之间形成一种头尾相接的循环等待资源关系

- 如何解除死锁

    1. 预防死锁

        通过设置一些限制条件，去产生破坏死锁的必要条件（比如设置timeout时间）

    2. 避免死锁

        在资源分配过程中，使用某种方法避免系统进入不安全的状态，从而避免死锁发生

    3. 以某种确定的顺序来获得锁

- Java实现死锁代码

    ```java
    public class DeadLock {
        public static void main(String[] args) {
            Object a = new Object();
            Object b = new Object();
            
            Thread threadA = new Thread(()->{
               synchronized (a) {
                   try {
                       System.out.println("get A");
                       Thread.sleep(1000);
                       synchronized (b) {
                           System.out.println("i wanna get B");
                       }
                   } catch (InterruptedException e) {
                       e.printStackTrace();
                   }
               }
            });
            
            Thread threadB = new Thread(()->{
                synchronized (b) {
                    try {
                        System.out.println("get B");
                        Thread.sleep(1000);
                        synchronized (a) {
                            System.out.println("i wanna get A");
                        }
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }
            });
            threadA.start();
            threadB.start();
        }
    }
    
    ```

- 死锁检查工具

    1. jstack命令


# ==--计算机网络==

## 一、TCP拥塞控制：慢开始、拥塞避免、快重传、快恢复

https://blog.csdn.net/sinat_21112393/article/details/50810053

- 拥塞控制：防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素
- 流量控制：指点对点通信量的控制，它要做的就是限制发送端发送数据的速率，使接收端能够来得及处理数据

**拥塞控制的方法：慢开始、拥塞避免、快重传和快恢复**

- 慢开始和拥塞避免

    - 慢开始算法

        发送方维持一个拥塞窗口cwnd的状态变量，拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞窗口（可将拥塞窗口就看作是发送窗口）。

        发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，窗口就再大一些，以便把更多的分组发送出去。但是只要网络出现拥塞，拥塞窗口就减小一些。一般是初始值设为1个MSS，然后每次加倍增长。

        “慢”不是说窗口的增长速率慢，而是开始时只发送一个报文段进行探测

        **为了防止拥塞窗口增长过大引起网络阻塞，还需要设置一个慢开始门限ssthresh状态变量：**

        - 当cwnd < ssthresh时，使用慢开始算法
        - 当cwnd > ssthresh时，使用拥塞避免算法
        - 当cwnd = ssthresh时，两种算法都可以使用

    - 拥塞避免算法

        让拥塞窗口缓慢地增大，没经过一个往返时间RTT就把发送方的拥塞窗口加1，而不是加倍。

    ==无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（没有收到确认），就要**把慢开始门限ssthresh设置为出现拥塞控制时发送方窗口值的一半**（不能小于2）。然后把拥塞窗口设置为1，再执行慢开始算法。这样做的目的就是要**迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕**。==

- 快重传和快恢复

    快重传算法首先要求接收方每收到一个失序的报文段后就立即发出重复确认（目的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时才进行捎带确认。

    ![image-20200715101617615](F:\java书和视频\笔记\images\面经\7.png)

    上图所示，发送方收到了4个对M2的确认消息，如果一连收到三个重复确认就应当立即重传对方尚未收到的报文段M3，而不必再等待接收对M3的确认消息

    快恢复算法配合快重传使用：

    1. 当发送方连续收到三个重复确认，就把慢开始门限ssthresh减半。这是为了预防网络发生拥塞。
    2. 由于发送方现在认为网络很可能没有发生拥塞，因此与慢开始不同之处在于是，现在不执行慢开始算法，而是将发送窗口值设定为ssthresh的值，然后执行拥塞避免算法，使发送窗口线性增大

在采用快恢复算法时，慢开始算法只是在**TCP连接建立时**和**网络出现超时**才使用

## 二、TCP粘包/拆包

TCP是一个“流”协议，数据没有一个明确的界限。TCP底层并不知道上层业务数据的具体含义，它只会根据TCP缓冲区的实际情况进行包的拆分，所以在业务上认为，一个完整的包可能会被TCP拆分成多个包进行发送，也有可能多个小包被封装成一个大的数据包进行发送。

### 1、TCP粘包/拆包问题说明

![image-20200715104333308](F:\java书和视频\笔记\images\面经\8.png)

假设客户端要发送D1和D2两个数据包给服务端，可能会发生以下几种情况：

1. D1和D2分两次完整独立的发送给了服务端，此时没有发生粘包和拆包现象
2. D1和D2太小被封装到了一个数据包中发送给服务端，此时发生了TCP粘包
3. 服务端分两次读取到了两个数据包，第一次读取到D1的全部和D2的一部分，第二次读取到D2的另一部分，发送了粘包和拆包
4. 服务端分两次读取到两个数据包，第一次读取到D1的一部分，第二次读取到D1的另一部分和D2的全部
5. 如果服务端的接收窗口很小，那么可能分多次才能完整收到D1和D2两个数据包

### 2、解决策略

1. 消息定长，例如每个报文都固定为200个字节，如果不够用空格补
2. 在包尾增加回车换行符进行分割，例如FTP协议
3. 传输消息时，附上消息的总长度

- Netty中的解决方法
    - `LineBasedFrameDecoder` 可以基于换行符解决。
    - `DelimiterBasedFrameDecoder`可基于分隔符解决。
    - `FixedLengthFrameDecoder`可指定长度解决。

# --操作系统

## 一、进程线程切换过程

- 进程切换

    1. 挂起一个进程，将这个进程在CPU中的状态（上下文）（CPU寄存器和程序计数器）存储在内存中的某处	（系统内核中）
    2. 在内存中检索下一个进程的上下文并将其在CPU的寄存器中恢复
    3. 跳转到程序计数器所指向的位置（即跳转到进程被中断时的代码行），以恢复该进程

    **注意：**进程切换只能发生在内核态，保存上下文和回复上下文的过程并不是免费的，需要内核在CPU上运行才能完成。

    用户进程在请求Redis，MySQL数据等网络IO阻塞掉的时候，或者在进程时间片到了，都会引发上下文切换

    **进程上下文切换有哪些开销**

    1. 直接开销

        直接开销就是在切换进程时CPU要做的事情：

        1. 切换页表全局目录
        2. 切换内核态堆栈
        3. 切换硬件上下文（进程恢复前，必须装入寄存器的数据统称为硬件上下文）

    2. 间接开销

        间接开销主要指的是虽然切换到一个新进程后，由于各种缓存并不热，速度运行会慢一些。如果进程始终都在一个CPU上调度还好一些，如果跨CPU的话，之前热起来的TLB、L1、L2、L3因为运行的进程已经变了，所以以局部性原理cache起来的代码、数据也都没有用了，导致新进程穿透到内存的IO会变多。 其实我们上面的实验并没有很好地测量到这种情况，所以实际的上下文切换开销可能比3.5us要大

- 线程切换

    **线程是调度的基本单位，而进程是拥有资源的基本单位**。所谓内核中的任务调度，实际上的调度对象其实是线程；而进程只是给线程提供了**虚拟内存**、全局变量等资源。所以对于线程和进程我们可以这样理解：

    - 当进程只有一个线程时，可以认为进程就等于线程
    - 当进程拥有多个线程时，这些线程会**共享相同的虚拟内存和全局变量**等资源。**这些资源在上下文切换时是不需要修改的**
    - 线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的

    线程的上下文切换可以分为两种情况：

    1. 前后两个线程**属于不同进程**。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样的
    2. 前后两个线程**属于同一个进程**。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，**只需要切换线程的私有数据、寄存器等不共享的数据。**

## 二、虚拟内存

程序寻址如果使用物理内存，那么寻址范围是有限的，这取决于CPU的地址线条数。比如在32位平台下，寻址的范围是$2^{32}$，也就是4G，并且这是固定的。

虚拟内存为每个进程提供了一个一致的，私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间），而实际上，它通常被分割成多个物理内存碎片，还有一部分存储在外部磁盘上，在需要时进行数据交换。

进程开始要访问一个地址，它可能会经历下面的过程：

1. 每次要访问地址空间中的某一个地址，都需要把地址翻译成实际物理内存地址
2. 所有进程都共享这一整个物理内存，每个进程只把自己**目前需要的虚拟地址空间映射到物理内存上**
3. 进程需要直到哪些地址空间上的数据在物理内存上，哪些不在，这就需要**页表**来记录
4. 页表的每一个表项分为两部分，第一部分记录此页是否在物理内存上，第二部分记录物理内存页的地址
5. 当进程访问某个虚拟地址时，就会先去看页表，如果发现对应的数据不在物理内存上，就会发生缺页中断
6. 发生缺页中断时，操作系统立即阻塞该进程，并将硬盘里的页换入内存，然后使该进程就绪；如果内存满了，就通过换页算法覆盖某一个页

https://juejin.im/post/59f8691b51882534af254317

https://blog.csdn.net/lvyibin890/article/details/82217193?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase

https://zhenbianshu.github.io/2018/11/understand_virtual_memory.html

## 三、中断

从本质上讲，中断(硬)是一种电信号，当设备有某种事情发生的时候，他就会产生中断，通过总线把电信号发送给中断控制器。如果中断的线是激活的，中断控制器就把电信号发送给处理器的某个特定引脚。处理器于是立即停止自己正在做的事，跳到中断处理程序的入口点，进行中断处理。

产生中断请求的设备或者事件被称为中断源，中断源可分为两类：一类是CPU内部中断，即执行软件中断指令INT或遇到软件陷阱而产生的中断，它们的中断类型号已由CPU规定好；另一类中断是由CPU以外的I/O设备产生的中断，又称硬件中断，硬件中断可分为不可屏蔽中断NMI和可屏蔽中断INTR，NMI用于紧急情况的故障处理，如RAM奇偶校验错等，INTR则用于外部依靠中断来工作的硬件设备。网卡使用的就是INTR

不可屏蔽中断源一旦提出请求，cpu必须无条件响应，而对于可屏蔽中断源的请求，cpu可以响应，也可以不响应。cpu一般设置两根中断请求输入线：可屏蔽中断请求INTR(Interrupt Require)和不可屏蔽中断请求NMI(Nonmaskable Interrupt)。对于可屏蔽中断，除了受本身的屏蔽位的控制外，还都要受一个总的控制，即CPU标志寄存器中的中断允许标志位IF(Interrupt Flag)的控制，IF位为1，可以得到CPU的响应，否则，得不到响应。IF位可以有用户控制，指令STI或Turbo c的Enable()函数，将IF位置1(开中断)，指令CLI或Turbo_c 的Disable()函数，将IF位清0(关中断)。典型的非屏蔽中断源的例子是电源掉电，一旦出现，必须立即无条件地响应，否则进行其他任何工作都是没有意义的。典型的可屏蔽中断源的例子是打印机中断，CPU对打印机中断请求的响应可以快一些，也可以慢一些，因为让打印机等待儿是完全可以的。

- 软中断所经过的操作比硬中断少吗？

    软中断过程：进程-》内核中的设备驱动程序

    硬中断过程：硬件-》CPU-》内核中的设备驱动程序

    产生软中断的进程一定是当前正在运行的进程，因此它们不会中断CPU，但是会中断调用代码的流程

- 硬中断与软中断的区别

    软中断是执行中断指令产生的，而硬中断是由外设引发的

    硬中断的中断信号是由中断控制器提供的，软中断的中断信号是由指令直接指出，无需使用中断控制器

    硬中断是可屏蔽的，软中断不可屏蔽

    硬中断处理程序要确保它能快速地完成任务，这样程序执行时才不会等待较长时间

    软中断处理硬中断未完成的工作

如果硬件需要CPU去做一些事情，那么这个硬件会使CPU中断当前正在运行的代码。而后CPU会将当前正在运行进程的当前状态放到堆栈（stack）中，以至于之后可以返回继续运行。**这种中断可以停止一个正在运行的进程；可以停止正处理另一个中断的内核代码；或者可以停止空闲进程。**

链接：https://www.jianshu.com/p/52a3ee40ea30

## 四、I/O多路复用

### 1. select函数

```c
int select(int maxfdp,fd_set *readfds,fd_set *writefds,fd_set *errorfds,struct timeval *timeout);
maxfdp : 需要监视的最大文件描述符加1。
readfds、writefds、errorfds：分别对应于需要检测的可读文件描述符的集合，可写文件描述符的集 合及异常文件描述符的集合。
timeout：等待时间，这个时间内，需要监视的描述符没有事件
发⽣生则函数返回，返回值为0。设为NULL 表示阻塞式等待，一直等到有事件就绪，函数才会返回，0表示非阻塞式等待，没有事件就立即返回，大于0表示等待的时间。
返回值：大于0表示就绪时间的个数，等于0表示timeout等待时间到了，小于0表示调用失败。
```

- select函数的原理

    select系统调用是用来让我们的程序监控多个文件句柄的状态变化的。**程序会停在select这里等待，直到被监视的文件句柄有一个或多个发生了状态改变**

    当用户process调用select的时候，select会将需要监控的readfds集合拷贝到内核空间（假设监控的仅仅是socket可读），然后遍历自己监控的socket sk，挨个调用sk的poll逻辑以便检查该sk是否有可读事件，遍历完所有的sk后，如果没有任何一个sk可读，那么select会调用schedule_timeout进入schedule循环，使得process进入睡眠。如果在timeout时间内某个sk上有数据可读了，或者等待timeout了，则调用select的process会被唤醒，接下来select就是遍历监控的sk集合，挨个收集可读事件并返回给用户了

- 优缺点

    优点：

    1. 可移植性好，在某些unix下不支持poll
    2. 对超时值提供了很好的精度，精确到微秒，而poll是毫秒

    缺点

    1. **单个进程可监视的fd数量被限制，默认是1024**
    2. 需要维护一个用来存放大量fd（文件描述符）的数据结构，这样会使得用户空间和内核空间在传递该结构的时候复制开销大
    3. **对fd进行扫描时是线性扫描，fd剧增后，IO效率降低，每次调用都对fd进行线性扫描遍历，随着fd的增加会造成遍历速度慢的问题**
    4. select函数超时参数在返回时也是未定义的，考虑到可移植性，每次超时之后进入下一个select之前都要重新设置超时参数

### 2.poll函数

```c
int poll ( struct pollfd * fds, unsigned int nfds, int timeout);
fds : 对应上述介绍的结构体指针
nfds : 标记数组中结构体元素的总个数。
timeout : 超时时间 ，等于0表示非阻塞式等待，小于0表示阻塞式等待，大于0表示等待的时间。
返回值：
成功时返回fds数组中事件就绪的文件描述符的个数
返回0表示超时时间到了。
返回-1表示调用失败，对应的错误码会被设置。
```

- 实现原理

    1. 将需要关心的文件描述符放进fds数组中
    2. 调用poll函数
    3. 函数成功返回后根据返回值遍历fds数组，将关心的事件与结构体中的revents相**与**判断事件是否就绪
    4. 事件就绪执行相关操作

    poll虽然解决了fds集合大小1024的限制问题，但是，它并没改变大量描述符数组被整体复制于用户态和内核态的地址空间之间，以及个别描述符就绪触发整体描述符集合的遍历的低效问题。poll随着监控的socket集合的增加性能线性下降，poll不适合用于大并发场景。

- 优缺点

    优点

    1. 不要求计算最大文件描述符+1的大小
    2. 应付大数量的文件描述符时比select快
    3. **没有最大连接数的限制，是基于链表存储的**

    缺点

    1. 大量的fd数组被整体复制于内核态和用户态之间，而不管这样的复制是不是有意义的
    2. **同select相同的是调用结束后需要轮询来获取就绪描述符**

### 3. epoll函数

1. 如何解决集合拷贝的问题

    对于IO多路复用，有两件事是必须要做的(对于监控可读事件而言)：1. 准备好需要监控的fds集合；2. 探测并返回fds集合中哪些fd可读了。细看select或poll的函数原型，我们会发现，每次调用select或poll都在重复地准备(集中处理)整个需要监控的fds集合。然而对于频繁调用的select或poll而言，fds集合的变化频率要低得多，我们没必要每次都重新准备(集中处理)整个fds集合。

    于是，epoll引入了epoll_ctl系统调用，将高频调用的epoll_wait和低频的epoll_ctl隔离开。同时，epoll_ctl通过(EPOLL_CTL_ADD、EPOLL_CTL_MOD、EPOLL_CTL_DEL)三个操作来分散对需要监控的fds集合的修改，做到了有变化才变更，将select或poll高频、大块内存拷贝(集中处理)变成epoll_ctl的低频、小块内存的拷贝(分散处理)，避免了大量的内存拷贝。同时，对于高频epoll_wait的可读就绪的fd集合返回的拷贝问题，epoll通过内核与用户空间mmap(内存映射)同一块内存来解决。mmap将用户空间的一块地址和内核空间的一块地址同时映射到相同的一块物理内存地址（不管是用户空间还是内核空间都是虚拟地址，最终要通过地址映射映射到物理地址），使得这块物理内存对内核和对用户均可见，减少用户态和内核态之间的数据交换。

    另外，epoll通过epoll_ctl来对监控的fds集合来进行增、删、改，那么必须涉及到fd的快速查找问题，于是，一个低时间复杂度的增、删、改、查的数据结构来组织被监控的fds集合是必不可少的了。在linux 2.6.8之前的内核，epoll使用hash来组织fds集合，于是在创建epoll fd的时候，epoll需要初始化hash的大小。于是epoll_create(int size)有一个参数size，以便内核根据size的大小来分配hash的大小。在linux 2.6.8以后的内核中，epoll使用红黑树来组织监控的fds集合，于是epoll_create(int size)的参数size实际上已经没有意义了。

2. 按需遍历就绪的fds集合

    通过上面的socket的睡眠队列唤醒逻辑我们知道，socket唤醒睡眠在其睡眠队列的wait_entry(process)的时候会调用wait_entry的回调函数callback，并且，我们可以在callback中做任何事情。为了做到只遍历就绪的fd，我们需要有个地方来组织那些已经就绪的fd。为此，epoll引入了一个中间层，一个双向链表(ready_list)，一个单独的睡眠队列(single_epoll_wait_list)，并且，与select或poll不同的是，epoll的process不需要同时插入到多路复用的socket集合的所有睡眠队列中，相反process只是插入到中间层的epoll的单独睡眠队列中，process睡眠在epoll的单独队列上，等待事件的发生。同时，引入一个中间的wait_entry_sk，它与某个socket sk密切相关，wait_entry_sk睡眠在sk的睡眠队列上，其callback函数逻辑是将当前sk排入到epoll的ready_list中，并唤醒epoll的single_epoll_wait_list。而single_epoll_wait_list上睡眠的process的回调函数就明朗了：遍历ready_list上的所有sk，挨个调用sk的poll函数收集事件，然后唤醒process从epoll_wait返回。


# ==--数据库==

## 一、MySQL

### 一、数据库中数据量过大，查询应该怎么处理

- 对于表的处理

    对数据库进行分库，对表进行分区，根据不同的业务以及数据特征，采用不同的分区方法

- 对于查询的处理

    B+树索引与位图索引相互配合

    采用并行处理

#### 对于数据库的拆分

如果数据库中由于表多而影响了性能，使用垂直切分，将数据库按业务拆分成不同的数据库，然后将不同的数据库放在多个服务器上。

#### 对于表的拆分

1. 垂直分表

    基于**列字段进行**，一般是表中的字段较多，将不常用的，数据较大，长度较长（比如**text**类型字段）的拆分到**“扩展表”**

2. 水平分表

    针对数据量巨大的单张表（比如订单表），按某种规则（**RANGE**，**HASH**取模等），拆分到多张表里面去。但是这些表还是再同一个库中，因此库级别的数据库操作还是有IO瓶颈，不**建议采用**

3. **水平分库分表**

    也可以将单张表的数据水平切分到多个服务器中去，每个服务器具有相应的库和表，只是**表中的数据集合不同**。水平分库分表能够有效的缓解单机和单库的性能瓶颈和压力。

4. 水平分库分表**切分规则**

    - RANGE

        从0到10000一个表，10000到20000一个表

    - HASH取模

        一个商场系统，一般都是将用户、订单作为主表，然后将和它们相关的作为副表，取用户id取hash，分不到不同的数据库上

    - 地理区域

    - 时间

### 二、数据库连接池的作用

#### 基本原理

对内部对象池中，维护一定数量的数据库连接，并对外暴露数据库连接的获取和返回方法

#### 作用

1. **资源重用**

    避免了频繁创建、释放连接引起的大量性能开销。在减少系统消耗的基础上，增进了系统环境的稳定性。

2. **更快的系统响应速度**

    连接池的初始化操作均已完成，业务的请求处理直接利用现有可用连接，避免了数据库连接初始化和释放过程的时间开销。

3. **新的资源分配手段**

### 三、InnoDB引擎数据存储

数据库中数据存储的基本单位是一张表中的一条记录，记录是按照 行 为单位存储的，但数据库加载磁盘数据到内存中并不是以 行 为单位。InnoDB中按照页来存储数据，默认情况下，一页大小为16KB。数据库从磁盘中加载数据到内存中，不论读一条数据还是多条数据，都是将整个页加载到内存中。

#### 1. 行记录在InnoDB中是如何进行存储的？

MySQL数据库中表的数据都是以“记录”为单位展示的，我们存储也是以记录为基本单位，也就是表中一条一条的数据。记录在磁盘上的存储方式被称为“行格式”或“记录格式”。

![image-20200717142540313](F:\java书和视频\笔记\images\面经\13.png)

一条记录数据的存储可以分为两个部分：“额外信息”和“真实数据”。额外信息用来描述记录，分为变长字段、Null值列表和记录头信息

- 变长字段：MySQL中一些字段的数据类型长度不固定，比如VARCHAR（n）或TEXT。这些变长字段的**真实长度**都需要在这里。各**变长字段数据占用的字节数按照列的顺序逆序存放**

- NULL值列表：记录中的某些列可能会存放null值，将这些null值都进行物理存储比较浪费空间，可以在null值列表里面存储这些列的标识，同样是按顺序逆序存放。

- 记录头信息：一共五个字节大小。

    ![image-20200717143238708](F:\java书和视频\笔记\images\面经\14.png)

数据库会为真实数据部分添加一些隐藏列，用于完成数据快速查找、事务提交、回滚等操作

| 列名           | 真实名称    | 是否必须 | 占用空间 | 描述               |
| -------------- | ----------- | -------- | -------- | ------------------ |
| roll_id        | DB_ROW_ID   | 否       | 6字节    | 唯一标识一条记录ID |
| transaction_id | DB_TRX_ID   | 是       | 6字节    | 事务ID             |
| roll_pointer   | DB_ROLL_PTR | 是       | 7字节    | 回滚指针           |

- DB_ROW_ID：我们知道它是可有可无的，这跟Innodb主键的生成策略有关。Innodb优先使用用户自定义主键作为主键，如果用户没有定义主键，则选取一个Unique键作为主键，如果表中连Unique键都没有定义的话，则InnoDB会为表默认添加一个名为row_id的隐藏列作为主键。为什么InnoDB非要生成主键呢？**因为数据库为了更好的进行范围查找和数据匹配，==总是将记录按照主键从小到大存储的==**，有序对于数据查找非常重要，记录存储有序了，就能高效使用**二分查找**策略了。
- DB_TRX_ID：用于记录记录的版本
- DB_ROLL_PTR：指向undo log中的某个节点用于回滚

#### 2. InnoDB中行记录是如何组织起来存储在页中的？

数据页的结构：

![image-20200717144003313](F:\java书和视频\笔记\images\面经\15.png)

- 使用File Header和File Tailer来界定一个页的范围。而且在File Tailer部分，存储了页面的校验和、日志序列位置（LSN），它们和File Header中存储的校验和与LSN相对应。Header 中还存储了该页**属于哪个表空间**，指向上一个页和下一个页的指针。

    在物理存储上，各个数据页之间不一定是顺序存放的；但是MySQL会尽可能保证在物理空间上连续。

- 行记录存储在User Records部分；初始页中并没有行记录，随着记录的增加，User Records就会占用Free Space，直到Free Space变为零。这时要继续增加记录，就会产生一个新的页。

    **在User Records中存储的记录是按照设定的主键大小顺序存放的，物理位置也是连续的。**

### 四、什么情况下索引会失效

2. like的模糊查询以%开头
3. 如果MySQL预计使用全表扫描要比使用索引快，则不使用索引
4. 如果索引列是可空的，是不会给其建立索引的
5. 条件上包括函数也不会走索引，因为索引在建立时和计算后可能不同

### 五、索引

#### 1. B+树和B树的区别

- B+树的中间节点不保存数据，而B树的中间节点要保存数据，**这使得B+树得层级更少**，**因为每个非叶子节点存储的关键字数更多**
- B+树查询必须 查询到叶子节点，B树只要匹配到即可不用管元素位置
- B+树的范围查询更简单，只需要遍历叶子节点链表即可，B树却需要重复的中序遍历

如果非关系型数据库单点查询比较多的话，使用B树则更合适

在InnoDB中采用B+树来建立索引。一个三层的B+树就可以标识上百万条数据。

一般来说索引本身也很大，因此不可能全部存储与内存中，而只是在需要的时候才从磁盘去取出数据到内存中进行处理。这样，在索引查找的过程中就会产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级（**磁盘访问有寻道时间和旋转时间**），所以评价一个数据结构作为索引的优劣的最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。

为了提高效率，磁盘往往不是严格按需存取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序往后读取一定长度的数据放入内存（**局部性原理**）。预读的长度一般为**页**的整数倍（计算机管理存储器的逻辑块）。

在InnoDB存储引擎中，记录也是按页（数据库中的页，大小和操作系统中的页相同）来进行存储的。这样在索引检索下，每次最多需要检索h个节点（h为树的高度），每个节点的大小就是一个页的大小。

在InnoDB中，数据文件本身就是一个索引文件。因为InnoDB会根据主键来建立一个**聚簇索引**，这个索引的叶子节点中存储着整行数据；如果没有设定主键，会检查是否有字段设定了唯一性约束，如果有就用这个字段建立聚簇索引；如果没有，那么就会生成一个隐藏字段，以该字段来建立聚簇索引。也就是说，**在InnoDB中，每一个表都至少并最多有一个聚簇索引**。也就可以将这个数据文件看作是一个索引文件。

由于只能有一个聚簇索引，如果要对其他字段建立索引，那么这些索引就叫作**辅助索引**。辅助索引的叶子节点不保存具体数据，而是保存该条记录相应主键的值。也就是说，如果要通过辅助索引查询整行数据，那么首先会走辅助索引，得到主键值和走聚簇索引，通过聚簇索引找到最终的数据。

#### 2. 索引建立的几大原则

- 最左前缀匹配原则。MySQL会一直向右匹配直到遇到范围查询（>,<,between,like)就停止匹配（**如果是第一列用范围查询，可以使用索引，但是后面的字段则不能**）。比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
- =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式
- 尽量选择区分度高的列作为索引
- 索引列不能参与计算
- 尽量扩展索引，而不要新建索引

#### 3. 索引选择性和前缀索引

索引文件本身要消耗存储空间，同时索引会加重插入、删除和修改记录时的负担；另外，MySQL在运行时也要消耗资源维护索引，因此索引并不是越多越好。

如果表中记录比较少，就没有必要建立索引。而且建立索引的列选择性不能很低。

### 六、事务

#### 1. 事务的概念

事务就是指一连串数据操作指令要么都做，要么都不做。会把数据库从一种一致状态转换为另一种一致状态。

#### 2. 事务的要求

ACID：原子性，一致性，隔离性，持久性

一致性:事务执行前后都是合法的数据状态，不会违背任何的数据完整性。事务开始和结束之间的中间状态不会被其他事务看到

一致性是指事务将数据从一种一致性状态转变为下一种一致性状态。==**在事务开始之前和事务结束之后，数据库的完整性约束没有被破坏。**==例如，在表中有一个字段为姓名，为唯一约束，即在表中姓名不能重复。如果一个事务对姓名段进行了修改，但是在事务提交或事务操作发生回滚后， 表中的姓名变得不唯一了，这就破坏了事务的一致性要求。

#### 3. 隔离性是如何实现的？

因为可能会有多个事务同时执行， 某个事务在执行的过程中，不能收到其他事务的影响。这就产生了隔离性的要求。隔离性是由锁来实现的。

事务在执行中可能会出现：脏读、不可重复读和幻读等问题，这些问题都是由于其他事务的影响造成的。由于不一定必须要完全解决这三种问题，所以数据库中出现了**事务隔离级别**的设置，设置不同的隔离级别可以解决不同的问题：Read Uncommitted、Read Committed、Repeatable Read、Serializable

如果是RU，则三种问题都不能解决；如果是RC，则可以解决脏读问题；RR则可以解决不可重复读问题，而在InnoDB存储引擎中，还能解决幻读问题；Serializable则可以解决全部问题。

RC和RR在MySQL中都是采用**MVCC**来解决的，MVCC就是存储了数据库行记录的多个版本，在RC模式下，事务会读MVCC存储的**最新的版本**；在RR模式下，事务会读取MVCC存储的事务开始时算起的**最老的版本。**

要解决幻读，在MySQL中是采用加next-key锁的方式。也就是即锁定当条记录，如果读取的是索引列的话，锁定该记录之前和之后的某一个范围。**如果没有索引的话，那么数据库会为整个表加上锁。**

**MySQL是如何实现MVCC的呢？**

==MVCC是通过在表中增加三个隐藏列来实现的。一个列保存了创建的时间（创建该条行记录时系统的版本号），另一列保存删除的时间（删除该条记录时系统的版本号），第三列为DB_ROLL_PTR，指向当前记录项的undo log记录，找之前的版本需要此指针。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号来进行比较。当然，还必须结合**undo log日志**==

![image-20200718163121374](F:\java书和视频\笔记\images\面经\17.png)

![image-20200718163159123](F:\java书和视频\笔记\images\面经\18.png)

- SELECT

    InnoDB会根据以下条件检查每行记录

    - InnoDB只查找版本早于当前事务版本的数据行（行的系统版本号小于或等于查询事务的版本号），这样就可以确保事务读取的行要么是在事务开始之前已经存在的，要么是事务自身插入或者修改过的。
    - 行的删除版本号要么未定义，要么大于当前事务版本号。这可以确保事务读取到的行，在事务开始之前没有被删除

- INSERT

    InnoDB为新插入的行保存当前系统版本号作为版本号。如果是新起的事务版本号就会递增。

- DELETE

    InnoDB为删除的每一行保存当前系统版本号作为行删除标识

- UPDATE

    更新操作也会插入一条新纪录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为删除标识
    
    

**MySQL中的锁又是如何实现的呢？**

**锁的等级**

- 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低
- 页面锁：开销和加锁时间位于表锁和行锁之间；会出现死锁；锁定粒度介于表锁和行锁之间；并发度一般
- 行锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度最高。

在数据库中，锁又分为**乐观锁**和**悲观锁**

- 乐观锁

    使用**数据版本**来实现的。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值是否相同，如果相同则可以更新，否则认为是过期数据。也可以用**时间戳**来实现。同样是在需要乐观锁控制的table中增加一个字段，字段类型使用时间戳，在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取得的时间戳是否相同。如果发现冲突了就让用户决定如何做。

- 悲观锁

    与Java的悲观锁类似。它是由数据库自己实现的，要用的时候直接调用数据库的相关语句。悲观锁又分为共享锁和排他锁。

    - 共享锁

        共享锁又可称为读锁，是读取操作创建的锁。其他事务可以并发读取数据，也可以再加共享锁，但是其他事务不能修改数据。加共享锁的语句为：`Lock in share mode`。**在查询语句后加这一句话，会对查询结果中的==每行==都加共享锁，当没有其他线程对查询结果中的任一行使用排他锁时，可以成功申请到锁，否则会被阻塞。**

    - 排他锁

        排他锁又可称作写锁。加上排他锁后，其他事务可以读加锁的记录，但是不能对其加上任何锁，也不能写记录。在需要执行的语句后加`for update`就可以使用排他锁。

- 行锁

    **InnoDB中，行锁是针对索引加的锁，不是针对记录加的锁。并且该索引不能失效，否则都会从行锁升级为表锁。**

    行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表锁。
    
    **==原因==**
    
    一个表至少且最多只有一个聚集索引，该聚集索引的叶子节点存储的数据是整个页，该页里面存储着许多行记录。如果加锁的字段没有索引，那么在查询该字段或者更新的时候都会全表扫描来找到匹配的字段，这样的话会对每一行都加行锁。会造成很大的开销，还不如直接加表锁。

#### 4. 原子性，一致性和持久性是怎么实现的？

原子性和持久性一般使用redo log（重做日志）来实现。一致性使用undo log（回滚日志）来实现。undo并不是redo的逆过程。

**redo log通常是物理日志，记录的是数据页的物理修改（也就是记录该行修改后的值），而不是某一行或某几行修改成怎样怎样，它用来恢复提交后的物理数据页（恢复数据页，且只能恢复到最后一次提交的位置）**

- 为什么需要Redo log

    在修改数据库中数据的时候，数据会先存在于内存中，再重写回磁盘中。如果在写回磁盘之前进程崩溃那么数据就丢失了。因此，提出使用redo log来先将要修改的数据写到这个日志文件中，并保证该日志文件比数据更早的写到磁盘中。这样在进程崩溃重启的时候，就可以根据redo log中的内容恢复数据。这也就保证了原子性和持久性

- redo log和二进制日志的区别
    1. 二进制日志是在存储引擎上层产生的，不管是什么存储引擎，对数据库进行了修改都会产生二进制日志文件。而redo  log是innodb层产生的，只记录该存储引擎中表的变化。
    2. 记录的内容不同。二进制日志记录的是逻辑更改情况，关于事务的具体操作内容；redo log记录的是关于每个页的更改的物理情况
    3. 写入的时间不同。二进制日志文件仅在事务提交前进行提交，只写磁盘一次；而在事务执行过程中会不断写redo log
    4. redo log是**循环写**，日志空间大小固定；binlog是追加写，一份日志文件写到一定大小的时候会更换下一个文件，不会覆盖
    5. binlog可以作为恢复数据使用，主从复制搭建；redo log作为异常宕机或者介质故障后的数据恢复备用

**undo log主要用于提供回滚和实现MVCC**

在数据修改的时候不仅记录了redo log，还记录了相应的undo log，如果因为某些原因导致事务失败或回滚了，可以使用undo log进行回滚。

**undo log是逻辑日志。可以认为当delete一条记录时，undo log中会记录一条对应的insert记录；当update一条记录时，它记录一条对应相反的update记录。**

### 七、死锁发生的情况和如何降低死锁的概率

#### 1.死锁发生的情况

![image-20200716214510292](F:\java书和视频\笔记\images\面经\9.png)

- 死锁案例一

    ![image-20200716214555744](F:\java书和视频\笔记\images\面经\10.png)

- 死锁案例二

    ![image-20200716214735206](F:\java书和视频\笔记\images\面经\11.png)

- 死锁案例三

    ![image-20200716214800680](F:\java书和视频\笔记\images\面经\12.png)

#### 2.如何降低死锁的概率

1. 案例一和三中，**对索引加锁顺序的不一致可能会导致死锁**。所以我们应该尽量以相同的顺序来访问索引记录和表。
2. 案例二中，由于默认情况下MySQL的隔离级别为RR，而且为了防止幻读会使用gap锁，如果能够确定幻读对应用的影响不大可以将隔离级别调为RC
3. 为表添加合理的索引
4. 避免大事务
5. 设置锁超时等待时间：`innodb_lock_wait_timeout`

### 八、一条SQL语句在MySQL中是如何执行的

![image-20200718150741529](F:\java书和视频\笔记\images\面经\16.png)

#### 1. 查询语句

```sql
select * from tb_student  A where A.age='18' and A.name='张三';
```

- 客户端发送一条查询语句，首先会经过连接器（它会完成与数据库的连接，并进行权限验证），如果没有权限那么就会直接返回错误信息；如果有权限，那么会先走缓存，看看缓存中是否有结果，没有的话才继续进行下一步
- 经过分析器，它提取语句中的关键数据，并完成对语句的词法、语法分析。如果没有问题则进行下一步
- 经过优化器，优化器会选择执行效率更好的一个方案，比如上述查询语句，可以选择先找名字等于”张三“的再找年龄等于18的，也可以反过来。
- 开始由存储引擎执行语句来查找数据

#### 2. 更新语句

```sql
update tb_student A set A.age='19' where A.name='张三';
```

开始的步骤和查询语句差不多，不过执行更新的时候会**记录日志**。mysql自带bin log（归档日志），所有存储引擎都可以使用，而InnoDB中还提供redo log（重做日志），用于帮助实现事务的原子性和持久性。具体操作过程：

- 先查询到张三这一条数据，如果有缓存，还是会走缓存，如果没在缓存中，就先从磁盘中读入内存，然后返回给**执行器**。
- 执行器拿到引擎给的行数据，把年龄改为19，然后调用引擎API接口，写入这一行数据。
- 引擎把这行数据更新到内存中，同时把这个更新操作记录到redo log中，此时redo log进入prepare状态。然后告诉**执行器**执行完成了，随时可以提交事务。（此时redo log文件中（不是缓冲）已经记录了操作的日志）
- 执行器收到通知后，生成这个操作的binlog，并把binlog写入磁盘。**此时事务还没有提交，也就是说在事务提交之前binlog已经写入了磁盘**
- 执行器调用引擎的事务提交接口，引擎把刚刚写入的redo log改为提交状态，更新完成。

**注意**

写入重做日志文件的操作不是直接写，而是先写入一个重做日志缓冲，然后按照一定的条件顺序地写入日志文件。从重做日志缓冲往磁盘写入时，是按512个字节，也就是一个扇区的大小进行写入。因为扇区是写入的最小单位，因此可以保证写入必定是成功的。因此在重做日志的写入过程中不需要有double write。

==即使某个事务还没有提交，InnoDB存储引擎仍然**每秒**会将重做日志缓冲中的内容刷新到重做日志文件中==。另一个触发写磁盘的过程是由`innodb_flush_log_at_trx_commit`控制的，表示在提交操作时，处理重做日志的方式。`innodb_flush_log_at_trx_commit`的有效值有0、1、2。0代表当提交事务时，并不将事务的重做日志缓冲写入磁盘上的日志文件，而是等待主线程每秒的刷新；1和2不同的地方在于：**1表示在执行commit时将重做日志缓冲同步到磁盘，即伴有fsync的调用**。2表示将重做日志缓冲异步写到磁盘，即写到文件系统的缓冲中。因此不能完全保证在执行commit时肯定会写入到重做日志文件，只是有这个动作发生。一般将值设为1来保证事务的ACID特性。

### 九、一条SQL语句执行很慢的原因有哪些？

- **如果大部分情况下执行速度正常，只是偶尔会很慢**

    当我们要往数据库插入一条数据、或者要更新一条数据的时候，数据库会更新**内存**中的数据，但是更新之后，这些更新的字段并不会马上写回磁盘，而是先写入到redo log当中，等到空闲的时候，再通过redo log里的日志把最新的数据同步到磁盘中去

    不过，redo log的容量有限制，一般是两个日志文件循环使用，如果数据库一直很忙，更新又很频繁，这个时候redo log很快就会被写满，这个时候就没办法等到空闲的时候再同步数据，只能暂停其他操作来将数据同步到磁盘中，而这个时候，就会导致SQL语句执行很慢。所以，**数据库在同步数据的时候，会导致SQL语句执行得慢**

    也有可能是执行这条语句的时候，访问的表被另外的线程加了锁，

- **这条SQL语句执行一直都很慢**

    有可能是SQL语句中查询的字段**没有建立索引**，或者索引失效了(模糊查询，运算符等)。再或者就是数据库选错了索引
    
    [为什么有索引查询也慢](https://www.acwing.com/blog/content/1687/)

### 十、数据库三大范式

- 第一范式：每个列都不可以再拆分

    1 NF是对属性的原子性要求，属性不可再分解

    如学生(学号、姓名、性别、出生年月日)，如果认为最后一列还可以细分为年、月、日，它就不满足第一范式，否则就满足

- 第二范式：在第一范式的基础上，非主键列**完全依赖于**主键，不能有任何一列与主键没有关系。要求每个表只描述一件事情

    约束对每一条记录的唯一性，要求记录有唯一标识，即实体的唯一性，即不存在部分依赖

    ```
    表:学号，课程号，姓名，学分
    ```

    这个表说明了两个事务：学生信息和课程信息；由于非主键字段必须依赖主键字段，这里学分依赖课程号，而姓名依赖学号，所以不满足第二范式

    **可能存在的问题**（增删查改都会存在问题）：

    - 数据冗余：每条记录都含有相同的信息
    - 删除异常：删除所有学生学分，就把课程信息全部删除了
    - 插入异常：学生未选课，无法进入数据库
    - 更新异常：调整课程学分，所有行都要调成

    **正确做法**：==添加一个中间关系表==

- 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。也就是说，每个表只描述一件事情的情况下， **每个字段都必须与主键直接相关，而不能间接相关**

    ```
    表：学号，学生，学院，学院电话
    ```

    这个表中学院电话依赖于学院，所以会存在信息冗余，比如很多学生都是一个学院，那么这个学院电话字段信息就产生了冗余

    **正确做法**：将学院和学院电话分离出来新建一个表

### 十一、InnoDB存储引擎与MyISAM引擎的区别

InnoDB存储引擎支持事务，支持外键，并且支持行锁，支持哈希索引；而MyISAM并不支持事务、外键、行锁哈希索引

**行锁与表锁的区别**

1. 表锁：开销小，加锁快；不会出现死锁（MyISAM会一次性获得SQL所需的全部锁）；锁定粒度大，发生锁冲突的概率最大，并发度最低。
2. 行锁：开销大，加锁慢；因为**行锁需要请求大量的锁资源，所以速度慢，内存消耗大。**

还有Memory引擎：所有数据都存储在内存中，处理速度快，但是安全性低

- 索引的区别

    1. InnoDB是聚簇索引，而MyISAM是**非聚簇索引**

    2. InnoDB主键索引的叶节点存储着行数据，因此主键索引非常高效

    3. **MyISAM的叶子节点存储的是行数据的地址，需要再寻址一次才能得到数据**

    4. InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到**覆盖索引**非常有效

        **覆盖索引**：一个查询语句的执行只用从索引中就能够得到想要的数据，不必从数据表中读取。如果一个索引包含了（或覆盖了）满足查询语句中字段与条件的数据就叫做覆盖索引

- 为什么MyISAM会比InnoDB的查询速度更快

    InnoDB在做select的时候，要维护的东西比MyISAM引擎多很多

    1）InnoDB 要缓存数据和索引，MyISAM只缓存索引块，这中间还有换进换出的减少

    2）innodb寻址要映射到块，再到行，MyISAM记录的直接是文件的OFFSET，**定位比INNODB要快**

    3）InnoDB 还需要维护MVCC一致；虽然你的场景没有，但他还是需要去检查和维护
    
- 是否保存数据库表中的具体行数

    InnoDB中不保存表的具体行数，也就是说，执行`select count(*) from table` 时，InnoDB要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可。

- 如何选择

    MyISAM适合:

    1. 做很多的count运算

    2. 插入不频繁，查询非常频繁，如果执行大量的select，MyISAM是更好的选择

    3. 没有事务

    4. **MyISAM 在执行查询语句(SELECT)前,会自动给涉及的所有表加读锁,在执行更新操作 (UPDATE、DELETE、INSERT 等)前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用LOCK TABLE命令给MyISAM表显式加锁。**

        

    InnoDB适合:

    1. 可靠性要求比较高，或者要求事务
    2. 表更新和查询都相当频繁，并且表锁定的机会比较大的情况指定数据引擎的创建； 
    3. 如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表；  （4）DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的 删除； 
    4. **InnoDB也会自动为数据加锁。对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁；对于普通SELECT语句，InnoDB不会加任何锁；当然我们也可以显示的加锁**

### 十二、数据库关系模型有哪三类完整性约束

1. 实体完整性

    实体完整性要求每个表都有唯一标识符，每一个表中的主键字段不能为空或者重复的值

2. 参照完整性

    参照完整性要求关系中不允许引用不存在的实体。

3. 用户自定义完整性

    用户自定义完整性是针对某一具体关系数据库的约束条件，它反映某一具体引用所涉及的数据必须满足的语义要求。

    1. 非空约束
    2. 唯一约束
    3. 检查约束
    4. 主键约束
    5. 外键约束

	### 十三、MySQL集群数据一致性

在“一主N备”的集群形式中，主要考虑master和slave的数据一致性

https://cloud.tencent.com/developer/article/1580170

## ==二、Redis==

### Redis的使用场景

1. 分布式缓存

    在分布式的系统架构中，将缓存存储在内存中无法与其他机器共享，使用Redis可以有效解决分布式缓存问题

2. 分布式锁

    在高并发的情况下，我们需要用所来防止并发带来的脏数据，Java自带的锁机制对进程间的并发不太好使，利用Redis单线程的特性可以实现分布式锁

3. Session存储/共享

    Redis可以将Session持久化到存储中，这样可以避免由于机器宕机而丢失用户会话信息

4. 发布/订阅

    可以设定一个key值进行消息发布以及消息订阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应的消息。

5. 任务队列

6. 限速，接口访问频率限制

### Redis有哪些数据结构

1. 字符串

    在Redis中是一个**可变的字节数组**，可以获取字串，获取长度，覆盖字串的内容， 追加字符串

2. 列表

    在Redis中的存储结构是**双向链表**。在**列表元素较少的时候**会使用一块连续的连续的内存存储，这个结构是**ziplist**，也就是**压缩列表**。它将所有的元素紧挨在一起存储，分配的是一块连续的内存空间。当数据量较多时，才会改成quicklist。因为普通链表需要的附加指针空间天大，除了要存储基本数据以外，还要存储只想前后的指针。Redis将链表和ziplist结合起来组成了quicklist，这样不会出现太大的空间冗余。

3. 哈希

    与Java中的HashMap类似

4. 集合

    与Java中的Set类似

5. 有序集合

    其底层的实现使用了两个数据结构，hash和跳跃列表，hash的作用就是关联元素value和权重score，保障value的唯一性，可以通过元素value找到相应的score。跳跃列表的目的在于给元素value排序，根据score的范围获取元素列表

### Redis在Spring Boot中的使用

1. 添加依赖

    ```java
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-redis</artifactId>
        <version>2.3.0.RELEASE</version>
    </dependency>
    ```

2. 配置

    ```yml
    Spring:
      redis:
        host: 192.168.1.107
        # Redis服务器的连接端口
        port: 6379
        # Redis的数据库索引
        database: 0
        jedis:
          pool:
            #连接池最大连接数
            max-active: 8
            #连接池最大阻塞等待时间,负值表示没有限制，无限等待直到超时为止
            max-wait: -1
            #连接池中的最大空闲连接，没有数据库连接时，依然可以保持最多8个空闲的连接不被清除
            max-idle: 8
            #连接池中的最小空闲连接，至少需要保持的空闲连接数，在初始化连接池的过程中就会建立min-idle个连接，在使用连接的过程中，如果连接数超过了min-idle，那么继续建立连接，但是不超过max-idle。设置为0，表示如果有空闲连接，那么空闲连接会在达到超时时间过后被清除
            min-idle: 0
        #连接超时时间，当客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能
        timeout: 0
    ```

    Spring Boot的`spring-boot-starter-data-redis`为Redis的相关操作提供了一个高度封装的RedisTemplate类，而且对每种类型的数据结构都进行了分类，将同一类型操作封装为operation接口。

    * 操作字符串：`redisTemplate.opsForValue()`
    * 操作 Hash：`redisTemplate.opsForHash()`
    * 操作 List：`redisTemplate.opsForList()`
    * 操作 Set：`redisTemplate.opsForSet()`
    * 操作 ZSet：`redisTemplate.opsForZSet()`

    对于String类型的操作，Spring Boot 还专门提供了 `StringRedisTemplate` 类，它和`RedisTemplate`的区别在于：

    1. `RedisTemplate`是一个泛型类，而`StringRedisTemplate`不是，后者只能对键和值都为 String 类型的数据进行操作，而前者则可以操作任何类型
    2. 两者的数据是不共通的，`StringRedisTemplate` 只能管理 `StringRedisTemplate` 里面的数据，`RedisTemplate` 只能管理 `RedisTemplate` 中 的数据

    官方建议使用`StringRedisTemplate` 

3. RedisTemplate的配置

    一个 Spring Boot 项目中，我们只需要维护一个 `RedisTemplate` 对象和一个 `StringRedisTemplate` 对象就可以了。所以我们需要通过一个 `Configuration` 类来初始化这两个对象并且交由的 `BeanFactory` 管理。

    ```java
    @Configuration
    public class RedisConfig {
        
        
        @Bean
        @ConditionalOnMissingBean(name = "redisTemplate")
        public RedisTemplate<String, Object> redisTemplate(
                RedisConnectionFactory redisConnectionFactory) {
    
            Jackson2JsonRedisSerializer<Object> jackson2JsonRedisSerializer = 
                    new Jackson2JsonRedisSerializer<Object>(Object.class);
            ObjectMapper om = new ObjectMapper();
            om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);
            om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL);
            jackson2JsonRedisSerializer.setObjectMapper(om);
    
            RedisTemplate<String, Object> template = new RedisTemplate<>();
            template.setConnectionFactory(redisConnectionFactory);
            template.setKeySerializer(jackson2JsonRedisSerializer);
            template.setValueSerializer(jackson2JsonRedisSerializer);
            template.setHashKeySerializer(jackson2JsonRedisSerializer);
            template.setHashValueSerializer(jackson2JsonRedisSerializer);
            template.afterPropertiesSet();
            return template;
        }
        
        @Bean
        @ConditionalOnMissingBean(StringRedisTemplate.class)
        public StringRedisTemplate stringRedisTemplate(
                RedisConnectionFactory redisConnectionFactory) {
            StringRedisTemplate template = new StringRedisTemplate();
            template.setConnectionFactory(redisConnectionFactory);
            return template;
        }
        
    }
    ```

4. 实现分布式锁

    ```java
    Boolean lockStat = stringRedisTemplate.execute((RedisCallback<Boolean>)connection ->
                        connection.set(key.getBytes(Charset.forName("UTF-8")), value.getBytes(Charset.forName("UTF-8")),
                                Expiration.from(timeout, timeUnit), RedisStringCommands.SetOption.SET_IF_ABSENT));
    ```

    ```java
    String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";
                boolean unLockStat = stringRedisTemplate.execute((RedisCallback<Boolean>)connection ->
                        connection.eval(script.getBytes(), ReturnType.BOOLEAN, 1,
                                key.getBytes(Charset.forName("UTF-8")), value.getBytes(Charset.forName("UTF-8"))));
    ```

### ==MySQL、Redis数据不一致如何解决==

对于读取缓存不会有什么问题，但是涉及到数据库的更新就存在数据库和缓存数据不一致的问题。不管是先删缓存再写数据库，还是先写数据库再删缓存都会有问题。

1. **如果线程A先删Redis缓存，还没有来得及更新数据库，此时线程B来查询数据，发现缓存为空，则去数据库中拿旧值，并且会将旧值写到缓存中。那么线程A更新数据库之后，缓存和数据库中的数据就不一致了。**
2. 如果线程A先写了数据库，在删除缓存前，写库的线程宕机了，没有删除掉缓存，也会出现数据不一致的状况。

* 第一种方案：采用延时双删策略

    在写库前后都进行缓存删除操作，并且第二次删除通过延迟的方式进行。具体步骤：

    1. 先删除缓存
    2. 再写数据库
    3. 休息一段时间
    4. 再次删除缓存

这么做的目的就是确保，读请求结束并给缓存添加脏数据后，写请求可以删除请求造成的缓存脏数据。
**为什么要进行双删**
数据库更新分为两个阶段，更新前删除缓存和更新后删除缓存，因为在数据库更新的过程中由于读操作存在并发可能，会出现缓存重新写入数据，因此需要更新后的删除。
**双删失败如何处理**

1. 设置缓存过期时间
   
    缓存设置过期时间是保证最终一致性的解决方案，结合双删策略+缓存超时设置，这样最差的情况就是在超时时间内数据存在不一致
    
2. 重试方案
   
        1) 更新数据库数据
        2) 数据库将操作信息写入binlog日志当中
        3) 订阅程序提取出所需要的数据以及key
        4) 另起一段非业务代码，获得该信息
        5) 尝试删除缓存操作，发现删除失败
        6) 将这些信息发送至消息队列
        7) 重新从消息队列中获得该数据，重试操作

- 第二种方案：加分布式锁

    | 写操作                                                       | 读操作                                                       |
    | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | 1.清除缓存；若失败则返回错误信息（本次写操作失败）。                               2.对key加分布式锁。                                                                                                     3.更新数据库；若失败则返回错误信息（本次写操作失败）同时释放锁，此时数据弱一致。                                                                                                                   4.更新缓存，即使失败也返回成功，同时释放锁，此时数据弱一致。 | 1.查询缓存，命中则直接返回结果。                                                                                                                      2.对key加分布式锁。                                                          3.查询数据库，将结果直接写入缓存，返回结果，同时释放锁。 |

### 缓存穿透问题

缓存穿透是指有恶意用户频繁请求缓存中不存在的数据，而对数据库造成大量的访问，导致数据库性能急剧下降，最终影响了服务的整体性能。解决方案：

1. 使用互斥锁排队，当从缓存中获取数据库失败时，给当前接口加上锁，从数据库中加载完数据并写入后再释放锁。若其他线程获取锁失败，则等待一段时间后重试
2. 将恶意用户请求的数据写入缓存中，并设置适当的过期时间
3. 使用布隆过滤器，将所有可能存在的数据存入布隆过滤器中，如果布隆过滤器返回数据不存在，那么此时就不不接收这次请求。

### 缓存雪崩问题

在短时间内有大量的缓存失效，如果这期间有大量的请求发生，那么这些请求就会跑到数据库，可能会导致数据库宕机。解决方案：

- 事前

    **使用集群缓存，保证缓存服务的高可用性**

- 事中

    **==ehcache本地缓存+Hystrix限流&降级，避免MySQL被打死==**

- 事后

    **开启Redis持久化机制，尽快恢复缓存集群**

1. **使用互斥锁排队。**
2. **为key设置不同的缓存失效时间**
3. 选择合适的内存淘汰策略

### 缓存击穿问题

缓存击穿与缓存雪崩不同，缓存雪崩是大量数据在同一时刻过期，而缓存击穿是数据库存在的某个数据，在缓存中过期时，刚好有大量请求来访问缓存中的数据，此时大量请求会到达数据库

解决方案：

1. **使用互斥锁**
2. **热点数据设置永不过期**

### 缓存预热

系统上线后，将相关的缓存数据直接加载到缓存系统，这样就可以避免在用户请求的时候先查询数据库

### ==Redis持久化机制==

Redis持久化方式：快照（RDB文件）和AOF

- RDB持久化方式会在一个特定的间隔保存那个时间点的一个数据快照
- AOF持久化方式则会**记录每个服务器收到的写操作**。在服务器启动时，这些记录的**操作会逐条执行**从而重建出原来的数据。写操作命令记录的格式跟Redis协议一致，以追加的方式进行保存

1. **RDB**（直接存数据的具体值）

    1. 工作原理
        - Redis调用fork()产生一个子进程
        - 子进程把数据写到一个临时的RDB文件
        - 当子进程写完新的RDB文件后，把旧的RDB文件替换掉
        
        所以，依据配置的RDB产生时间，那么如果服务器崩掉的话，会有最近的一段时间的数据会丢失
        
        ```xml
        save 900 1 : 900秒后，且至少数据有一次变更就执行RDB快照持久化
        save 300 10
        save 60 10000
        ```
        
    2. 保存（rdbSave）

        rdbSave函数负责将内存中的数据库数据以rdb格式保存到磁盘上，如果rdb文件已经存在，那么新的rdb文件将替换已有的rdb文件

        **在保存rdb文件期间，主进程会被阻塞，直到保存完成为止**

        save和bgsave两个命令都会调用rdbSave，但它们调用的方式各有不同

        - save：直接调用rdbSave，阻塞Redis主进程，直到保存完成为止，在主进程阻塞期间，服务器不能处理客户端的任何请求。

            在save执行过程中，save，bgsave，bgrewriteaof调用都不会产生任何作用。只有上一个save命令完成过后，后面的命令才能开始执行。

        - bgsave： fork一个子进程，子进程负责调用rdbSave，并在保存完成之后向主进程发出信号，通知保存已完成。因为rdbSave在子进程被调用，所以redis服务器在bgSave执行期间仍然可以继续处理客户端的请求

            save和bgsave不能同时执行，避免造成竞态条件，bgsave和bgsave也不能同时执行。

            bgsave和bgrewriteaof也不能同时进行，虽然这两个命令在操作方面并没有什么冲突，不能同时执行是对性能方面的考虑：并发出两个子进程进行大量的磁盘操作影响性能

    3. 载入

        当Redis服务器启动的时候，rdbLoad函数就会被执行，它读取rdb文件，并将文件中的数据载入到内存中。在载入期间，每载入1000个键就处理一次所有已到达的请求，不过只有publish、subscribe、psubscribe、unsbscribe相关指令会被正确地处理，其他命令一律返回错误。等待载入完成之后，服务器才开始正常处理所有命令。

        **AOF文件的保存频率通常要高于RDB文件保存的频率，所以一般来说，AOF文件中的数据会比RDB文件中的数据更新。因此，如果服务器在启动时，打开了AOF功能，那么程序优先使用AOF文件来还原数据。只有在AOF功能未打开的情况下，Redis才会使用RDB文件来还原数据**

    4. 优点

        适用于disaster recovery，和备份，因为rdb文件很紧凑并且保存了某个时间点的数据

        可以最大化Redis性能，因为父进程需要做的只是fork一个子进程

    5. 缺点

        会产生长时间的数据丢失 

        在数据集比较大的情况下，fork可能会非常耗时，fork时主进程会被阻塞，因此不能响应客户请求

2. **AOF**（**以协议文本的方式，将所有对数据库进行过写入的命令及其参数记录到AOF文件**）

    1. AOF命令同步

        如果执行以下命令：

        ```shell
    redis> RPUSH list 1 2 3 4
        (integer) 4
    
        redis> LRANGE list 0 -1
    1) "1"
        2) "2"
        3) "3"
        4) "4"
    
        redis> KEYS *
    1) "list"
        
        redis> RPOP list
        "4"
        
        redis> LPOP list
        "1"
        
        redis> LPUSH list 1
        (integer) 3
        
        redis> LRANGE list 0 -1
        1) "1"
        2) "2"
        3) "3"
        ```
    
        那么其中4条对数据库有修改的写入命令就会被同步到AOF文件中:
    
        ```shell
        RPUSH list 1 2 3 4
        
        RPOP list
        
        LPOP list
        
        LPUSH list 1
        ```
    
        为了处理的方便， AOF 文件使用网络通讯协议的格式来保存这些命令。
    
        同步命令到AOF文件的整个过程可以分为三个阶段：
    
        1. 命令传播：Redis将执行完的命令、命令的参数、命令的参数的个数等信息发送到**AOF程序（另一个进程）**中
        2. 缓存追加：AOF程序根据接收到的命令数据，将命令转换为网络通讯协议的格式，然后将协议内容追加到服务器的**AOF缓存**中
        3. 文件写入和保存：AOF缓存中的内容被写入到AOF文件末尾，如果设定的AOF保存条件被满足的话，fsync函数或者fdatasync函数会被调用，将写入的内容真正地保存到磁盘中
    
    2. **命令传播**
    
        当一个Redis客户端需要执行命令时，它通过网络连接，将协议文本发送给Redis服务器。
    
        比如说， 要执行命令 `SET KEY VALUE` ， 客户端将向服务器发送文本 `"*3\r\n$3\r\nSET\r\n$3\r\nKEY\r\n$5\r\nVALUE\r\n"` 。
    
        服务器在接到客户端的请求之后， 它会根据协议文本的内容， 选择适当的命令函数， 并将各个参数从字符串文本转换为 Redis 字符串对象（`StringObject`）。
    
        比如说， 针对上面的 [SET](http://redis.readthedocs.org/en/latest/string/set.html#set) 命令例子， Redis 将客户端的命令指针指向实现 [SET](http://redis.readthedocs.org/en/latest/string/set.html#set) 命令的 `setCommand` 函数， 并创建三个 Redis 字符串对象， 分别保存 `SET` 、 `KEY` 和 `VALUE` 三个参数（命令也算作参数）。
    
        每当命令函数成功执行之后， 命令参数都会被传播到 AOF 程序， 以及 REPLICATION 程序（本节不讨论这个，列在这里只是为了完整性的考虑）。
    
    ![image-20200730201215906](F:\java书和视频\笔记\images\面经\28.png)

 3. 缓存追加

    当命令被传播到AOF程序之后，程序会根据命令以及命令的参数，将命令从字符串对象转换回原来的协议文本。

    比如说， 如果 AOF 程序接受到的三个参数分别保存着 `SET` 、 `KEY` 和 `VALUE` 三个字符串， 那么它将生成协议文本 `"*3\r\n$3\r\nSET\r\n$3\r\nKEY\r\n$5\r\nVALUE\r\n"` 。

    协议文本生成之后， 它会被追加到 `redis.h/redisServer` 结构的 `aof_buf` 末尾。

    `redisServer` 结构维持着 Redis 服务器的状态， `aof_buf` 域则保存着所有等待写入到 AOF 文件的协议文本：

	4. 文件的写入和保存

    每当服务器常规任务函数被执行，或者事件处理器被执行时，`aof.c/flushAppendOnlyFile` 函数都会被调用， 这个函数执行以下两个工作：

    **WRITE：根据条件，将 `aof_buf` 中的缓存写入到 AOF 文件。**

    **SAVE：根据条件，调用 `fsync` 或 `fdatasync` 函数，将 AOF 文件保存到磁盘中。**

    两个步骤都需要根据一定的条件来执行， 而这些条件由 AOF 所使用的保存模式来决定， 以下小节就来介绍 AOF 所使用的三种保存模式， 以及在这些模式下， 步骤 WRITE 和 SAVE 的调用条件。

	5. AOF保存模式

    Redis 目前支持三种 AOF 保存模式，它们分别是：

    1. `AOF_FSYNC_NO` ：不保存。
    2. `AOF_FSYNC_EVERYSEC` ：每一秒钟保存一次。
    3. `AOF_FSYNC_ALWAYS` ：每执行一个命令保存一次。

    - 不保存

        **在这种模式下， 每次调用 `flushAppendOnlyFile` 函数， WRITE 都会被执行， 但 SAVE 会被略过。**

        在这种模式下， SAVE 只会在以下任意一种情况中被执行：

        - Redis 被关闭
        - AOF 功能被关闭
        - 系统的写缓存被刷新（可能是缓存已经被写满，或者定期保存操作被执行）

        这三种情况下的 **SAVE 操作都会引起 Redis 主进程阻塞**。

    - 每秒钟保存一次

        在这种模式中， SAVE 原则上每隔一秒钟就会执行一次， 因为 SAVE 操作是由后台子线程调用的， 所以它不会引起服务器主进程阻塞。

        注意， 在上一句的说明里面使用了词语“原则上”， 在实际运行中， 程序在这种模式下对 `fsync` 或 `fdatasync` 的调用并不是每秒一次， 它和调用 `flushAppendOnlyFile` 函数时 Redis 所处的状态有关。

        每当 `flushAppendOnlyFile` 函数被调用时， 可能会出现以下四种情况：

        ![image-20200730202007053](F:\java书和视频\笔记\images\面经\29.png)

    

6. AOF重写

    AOF文件的大小会随时间不断增大，Redis服务器可以创建一个新的AOF文件来替换现有的AOF文件，新旧两个文件所保存的数据库状态是相同的，但是新的AOF文件不会包含任何浪费空间的冗余命令

    实现原理是：**首先从数据库中读取键现在的值，然后用一条命令去记录键值对，代替之前记录该键值对的多个命令**

    Redis是采用**新增子进程**来进行AOF重写的，但是在子进程进行AOF重写期间，服务器进程还会继续处理命令请求，而新的命令可能会对现有的数据进行修改，这会让当前数据库的数据和重写后的AOF文件中的数据不一致

    为了解决这种数据不一致问题，**Redis增加了一个AOF重写缓存**，**这个缓存在fork出子进程之后开始启用**。**Redis主进程在执行完写命令之后，会同时将这个写命令追加到AOF缓冲区和AOF重写缓冲区**

    子进程在执行AOF重写时，主进程需要进行以下三个工作：

    - 执行client发送的请求命令
    - 将写命令追加到现有的AOF文件中（旧的AOF文件）
    - 将写命令追加到AOF重写缓冲区

    子进程完成AOF重写后，会向父进程发送一个完成信号，父进程接收到该完成信号之后，会调用一个信号处理函数，该函数完成以下工作：

    - 将AOF重写缓存中的内容全部追加到新的AOF文件中
    - 对新的AOF文件进行改名，覆盖原有的AOF文件

7. AOF后台重写的触发条件

    可有用户通过`bgrewriteaof`手动触发

    

### Redis事务

Redis通过Multi、Discard、Exec、和Watch四个命令来实现事务功能。以Multi开始一个事务，然后将多个命令入队到事务中，最后由Exec命令触发事务，一并执行事务队列中的所有命令。而**Watch是实现乐观锁的机制**。

一个事务从开始到结束会经历以下三个阶段：

1. 开始事务
2. 命令入队
3. 执行事务

- 开始事务

    Multi命令的执行标记着事务的开始，这个命令唯一做的就是，将客户端的==`redis_multi`==选项打开，让客户端从非事务状态切换到事务状态

- 命令入队

    当客户端处于非事务状态时，所有发送给服务器的命令会立即被执行。但是，当客户端进入到事务状态时，服务器在收到来自客户端的命令时，不会立即执行命令，而是将这些命令全部放到一个事务队列中，然后返回Queued，标识命令已经入队

    ![image-20200730215050345](F:\java书和视频\笔记\images\面经\30.png)

事务队列就是一个数组，数组中的每个元素是一个标识命令的结构体，有命令名、命令参数、参数个数这三个属性。

- 执行事务

    ![image-20200730215454962](F:\java书和视频\笔记\images\面经\31.png)

**Discard命令**

Discard命令取消一个事务，**它清空客户端的整个事务队列，然后将客户端从事务状态调整回非事务状态**

**Redis 的事务是不可嵌套的**， 当客户端已经处于事务状态， 而客户端又再向服务器发送 [MULTI](http://redis.readthedocs.org/en/latest/transaction/multi.html#multi) 时， 服务器只是简单地向客户端发送一个错误， 然后继续等待其他命令的入队。 [MULTI](http://redis.readthedocs.org/en/latest/transaction/multi.html#multi) **命令的发送不会造成整个事务失败， 也不会修改事务队列中已有的数据。**

[WATCH](http://redis.readthedocs.org/en/latest/transaction/watch.html#watch) **只能在客户端进入事务状态之前执行**， 在事务状态下发送 [WATCH](http://redis.readthedocs.org/en/latest/transaction/watch.html#watch) 命令会引发一个错误， 但它不会造成整个事务失败， 也不会修改事务队列中已有的数据（和前面处理 [MULTI](http://redis.readthedocs.org/en/latest/transaction/multi.html#multi) 的情况一样）。

**带Watch的事务**

==Watch用于在事务开启之前监控任意数量的键，如果事务提交过后，发现已经有某个被监控的键被改变了，那么整个事务就不会执行== 

**==Watch命令的实现==**

在每个代表数据库的 `redis.h/redisDb` 结构类型中， 都保存了一个 `watched_keys` 字典， 字典的键是这个数据库被监视的键， 而字典的值则是一个链表， **链表中保存了所有监视这个键的客户端**。

![image-20200730220832140](F:\java书和视频\笔记\images\面经\32.png)

其中， 键 `key1` 正在被 `client2` 、 `client5` 和 `client1` 三个客户端监视， 其他一些键也分别被其他别的客户端监视着。

[WATCH](http://redis.readthedocs.org/en/latest/transaction/watch.html#watch) 命令的作用， 就是将当前客户端和要监视的键在 `watched_keys` 中进行关联。

如果当前客户端为 `client10086` ， 那么当客户端执行 `WATCH key1 key2` 时， 前面展示的 `watched_keys` 将被修改成这个样子：

![image-20200730220932726](F:\java书和视频\笔记\images\面经\33.png)

**Watch的触发**

在任何对数据库键空间（key space）进行修改的命令成功执行之后 （比如 [FLUSHDB](http://redis.readthedocs.org/en/latest/server/flushdb.html#flushdb) 、 [SET](http://redis.readthedocs.org/en/latest/string/set.html#set) 、 [DEL](http://redis.readthedocs.org/en/latest/key/del.html#del) 、 [LPUSH](http://redis.readthedocs.org/en/latest/list/lpush.html#lpush) 、 [SADD](http://redis.readthedocs.org/en/latest/set/sadd.html#sadd) 、 [ZREM](http://redis.readthedocs.org/en/latest/sorted_set/zrem.html#zrem) ，诸如此类）， `multi.c/touchWatchedKey` 函数都会被调用 —— 它检查数据库的 `watched_keys` 字典， 看是否有客户端在监视已经被命令修改的键， 如果有的话， 程序将所有监视这个/这些被修改键的客户端的 `REDIS_DIRTY_CAS` 选项打开：

![image-20200730221517315](F:\java书和视频\笔记\images\面经\34.png)

当客户端发送 [EXEC](http://redis.readthedocs.org/en/latest/transaction/exec.html#exec) 命令、触发事务执行时， 服务器会对客户端的状态进行检查：

- 如果客户端的 `REDIS_DIRTY_CAS` 选项已经被打开，那么说明被客户端监视的键至少有一个已经被修改了，事务的安全性已经被破坏。服务器会放弃执行这个事务，直接向客户端返回空回复，表示事务执行失败。
- 如果 `REDIS_DIRTY_CAS` 选项没有被打开，那么说明所有监视键都安全，服务器正式执行事务。



**==在事务提交之前，如果Redis进程被终结会造成什么后果？==**

如果 Redis 服务器进程在执行事务的过程中被其他进程终结，或者被管理员强制杀死，那么根据 Redis 所使用的持久化模式，可能有以下情况出现：

- 内存模式：如果 Redis 没有采取任何持久化机制，那么重启之后的数据库总是空白的，所以数据总是一致的。

- RDB 模式：**在执行事务时，Redis 不会中断事务去执行保存 RDB 的工作，只有在事务执行之后，保存 RDB 的工作才有可能开始**。所以当 RDB 模式下的 Redis 服务器进程在事务中途被杀死时，事务内执行的命令，不管成功了多少，都不会被保存到 RDB 文件里。恢复数据库需要使用现有的 RDB 文件，而这个 RDB 文件的数据保存的是最近一次的数据库快照（snapshot），所以它的数据可能不是最新的，但只要 RDB 文件本身没有因为其他问题而出错，那么还原后的数据库就是一致的。

- ==**AOF 模式**==：**因为保存 AOF 文件的工作在后台线程进行，所以即使是在事务执行的中途，保存 AOF 文件的工作也可以继续进行，因此，根据事务语句是否被写入并保存到 AOF 文件，有以下两种情况发生**：

    1）**如果事务语句未写入到 AOF 文件，或 AOF 未被 SYNC 调用保存到磁盘，那么当进程被杀死之后，Redis 可以根据最近一次成功保存到磁盘的 AOF 文件来还原数据库，只要 AOF 文件本身没有因为其他问题而出错，那么还原后的数据库总是一致的，但其中的数据不一定是最新的。**

    2）**如果事务的部分语句被写入到 AOF 文件，并且 AOF 文件被成功保存，那么不完整的事务执行信息就会遗留在 AOF 文件里，当重启 Redis 时，程序会检测到 AOF 文件并不完整，Redis 会退出，并报告错误。需要使用 ==redis-check-aof== 工具将部分成功的事务命令移除之后，才能再次启动服务器。还原之后的数据总是一致的，而且数据也是最新的（直到事务执行之前为止）。**

### Redis集群

Redis集群是一个提供在多个Redis节点间共享数据的程序集

Redis集群不支持处理多个keys的命令，因为这可能在不同的节点间移动数据，从而达不到像Redis一样的高性能，在高负载的情况下可能会导致e不可预料的错误

Redis集群通过分区来提供一定程度的可用性，在实际环境中，当某个节点宕机或者不可达的情况下继续处理命令。

Redis集群的优势:

1. 自动分割数据到不同的节点上
2. 整个集群的部分节点失败或者不可达的情况下能够继续执行命令

#### Redis 集群的数据分片

Redis 集群没有使用一致性hash, 而是引入了 **哈希槽**的概念.

Redis 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽,举个例子,比如当前集群有3个节点,那么:

- 节点 A 包含 0 到 5500号哈希槽.
- 节点 B 包含5501 到 11000 号哈希槽.
- 节点 C 包含11001 到 16384号哈希槽.

这种结构很容易添加或者删除节点. 比如如果我想新添加个节点D, 我需要从节点 A, B, C中得部分槽到D上. 如果我想移除节点A,需要将A中的槽移到B和C节点上,然后将没有任何槽的A节点从集群中移除即可. 由于从一个节点将哈希槽移动到另一个节点并不会停止服务,所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态.

#### Redis 集群的主从复制模型

为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型,每个节点都会有N-1个复制品.

在我们例子中具有A，B，C三个节点的集群,在没有复制模型的情况下,如果节点B失败了，那么整个集群就会以为缺少5501-11000这个范围的槽而不可用.

然而如果在集群创建的时候（或者过一段时间）我们为每个节点添加一个从节点A1，B1，C1,那么整个集群便有三个master节点和三个slave节点组成，这样在节点B失败后，集群便会选举B1为新的主节点继续服务，整个集群便不会因为槽找不到而不可用了

不过当B和B1 都失败后，集群是不可用的.

#### Redis 一致性保证

Redis 并不能保证数据的**强一致性**. 这意味这在实际中集群在特定的条件下可能会丢失写操作.

第一个原因是因为集群是用了异步复制. 写操作过程:

- 客户端向主节点B写入一条命令.
- 主节点B向客户端回复命令状态.
- 主节点将写操作复制给他得从节点 B1, B2 和 B3.

主节点对命令的复制工作发生在返回命令回复之后， 因为如果每次处理命令请求都需要等待复制操作完成的话， 那么主节点处理命令请求的速度将极大地降低 —— 我们必须在性能和一致性之间做出权衡。 注意：Redis 集群可能会在将来提供同步写的方法。 Redis 集群另外一种可能会丢失命令的情况是集群出现了网络分区， 并且一个客户端与至少包括一个主节点在内的少数实例被孤立。

举个例子 假设集群包含 A 、 B 、 C 、 A1 、 B1 、 C1 六个节点， 其中 A 、B 、C 为主节点， A1 、B1 、C1 为A，B，C的从节点， 还有一个客户端 Z1 假设集群中发生网络分区，那么集群可能会分为两方，大部分的一方包含节点 A 、C 、A1 、B1 和 C1 ，小部分的一方则包含节点 B 和客户端 Z1 .

Z1仍然能够向主节点B中写入, 如果网络分区发生时间较短,那么集群将会继续正常运作,如果分区的时间足够让大部分的一方将B1选举为新的master，那么Z1写入B中得数据便丢失了.

注意， 在网络分裂出现期间， 客户端 Z1 可以向主节点 B 发送写命令的最大时间是有限制的， 这一时间限制称为节点超时时间（node timeout）， 是 Redis 集群的一个重要的配置选项

## ==三、分布式锁的应用与实现原理==

### 基于数据库实现分布式锁

- 基于数据库表

    要实现分布式锁，最简单的方式就是直接创建一张锁表，然后通过操作表中的数据来实现分布式锁。

    **当我们锁住某个方法或资源时，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录**

    ```sql
    CREATE TABLE `methodLock` (
      `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键',
      `method_name` varchar(64) NOT NULL DEFAULT '' COMMENT '锁定的方法名',
      `desc` varchar(1024) NOT NULL DEFAULT '备注信息',
      `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间，自动生成',
      PRIMARY KEY (`id`),
      UNIQUE KEY `uidx_method_name` (`method_name `) USING BTREE
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='锁定中的方法';
    ```

    我们想要**锁住某个方法时**，执行以下SQL：

    ```sql
    insert into methodLock(method_name,desc) values (‘method_name’,‘desc’)
    ```

    因为我们对`method_name`做了**唯一性约束**，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的线程获得了该方法的锁，可以执行方法体

    当要释放锁时，执行以下sql语句：

    

```sql
delete from methodLock where method_name ='method_name'
```

**问题：**

1. 这把锁强依赖于数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用

2. 锁没有失效时间，一旦解锁操作失败就会导致锁记录一直在表中，其他线程无法再获得锁

3. 这把锁只能是非阻塞的，因为一旦insert操作失败就会直接报错，线程无法进入排队队列，要想再次获得锁就必须再次触发获得锁操作

4. 这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得锁

    

- 基于数据库排他锁

    使用之前的数据库表，通过数据库的排他锁可以实现分布式锁

    ```java
    public boolean lock(){
        connection.setAutoCommit(false)
        while(true){
            try{
                result = select * from methodLock where method_name=xxx for update;
                if(result==null){
                    return true;
                }
            }catch(Exception e){
    
            }
            sleep(1000);
        }
        return false;
    }
    ```

    在查询语句后面加上`for update`，数据库会在查询过程中给数据库表添加排他锁（**InnoDB存储引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁**。因此要想使用行级锁，必须给`method_name`添加索引，而且这个索引一定要创建成**唯一索引**，否则会出现多个重载方法无法同时被访问的问题）。当某条记录被加上排他锁，其他线程无法再在该行记录上增加排他锁。

    这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。

    1. ​	`for update`语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功
    2. 锁定之后服务宕机，数据库会把锁释放掉

    但是还有一个问题：**如果索引失效，那么数据库会为该表添加上表锁。而且，如果排他锁长时间不提交，就会一直占用数据库连接。**

    **数据库实现分布式锁的优点**：直接借助数据库，容易理解

    **数据库实现分布式锁的缺点**

    1. 复杂
    2. 有开销，性能问题
    3. 使用数据库的行级锁不一定靠谱，可能会变为使用表锁

### 基于缓存实现分布式锁

该方法比用数据库在性能方面更好，而且缓存可以集群部署，能够解决单点问题。

使用Redis实现分布式锁可以采用如下方法:

```java
   public static boolean tryGetDistributedLock(Jedis jedis, String lockKey, String requestId, int expireTime) {
        String result = jedis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime);
        if (LOCK_SUCCESS.equals(result)) {
            return true;
        }
        return false;
    }
```

- 第一个为key，使用key来当锁，因为key是唯一的
- 第二个为value，分布式锁需要特定的客户端来解锁，通过给value赋值为requestId，就知道是哪个客户端加的锁。value需要指定为唯一指，通常使用`UUID.randomUUId().toString()`来指定
- 第三个为NX，意思是SET_IF_NOT_EXIST，即当key不存在的时候进行set操作，若key已经存在，则不做任何操作
- 第四个为expx，表明我们要给key指定一个过期时间，具体时间由第五个参数指定
- 第五个为time，代表具体的过期时间

解锁操作：

```java
public class RedisTool {

    private static final Long RELEASE_SUCCESS = 1L;

    /**
     * 释放分布式锁
     * @param jedis Redis客户端
     * @param lockKey 锁
     * @param requestId 请求标识
     * @return 是否释放成功
     */
    public static boolean releaseDistributedLock(Jedis jedis, String lockKey, String requestId) {

        String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";
        Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));

        if (RELEASE_SUCCESS.equals(result)) {
            return true;
        }
        return false;

    }

}
```

**注意**：

1. **不能使用`jedis.setnx()`和`jedis.expire()`组合实现加锁，因为如果用两条命令来实现锁，不能保证加锁操作的原子性。**
2. **一定要给锁指定一个唯一的标识**
3. **解锁的时候要使用lua脚本，它可以保证解锁操作的原子性**
4. **不能使用`jedis.del()`直接删除锁，必须要先判断锁的拥有者**

- Redisson实现Redis分布式锁的底层原理

    ![image-20200714102843602](F:\java书和视频\笔记\images\面经\1.png)

    1. 加锁机制

        某个客户端要加锁，如果客户端面对的是一个Redis集群，它会首先根据hash节点选择**一台机器**。然后发送一段lua脚本到这台机器上

        ![image-20200714103118515](F:\java书和视频\笔记\images\面经\2.png)

        ​	

        那么，这段lua脚本是什么意思呢？这里**KEYS[1]**代表的是你加锁的那个key，比如说：`RLock lock = redisson.getLock("myLock");`这里你自己设置了加锁的那个锁key就是`myLock`。

        `ARGV[1]`代表的就是锁key的默认生存时间，默认30秒。`ARGV[2]`代表的是加锁的客户端的ID，类似于下面这样：`8743c9c0-0795-4907-87fd-6c719a6b4586:1`

        给大家解释一下，第一段if判断语句，就是用`“exists myLock”`命令判断一下，如果你要加锁的那个锁key不存在的话，你就进行加锁。如何加锁呢？很简单，**用下面的命令**：`hset myLock 8743c9c0-0795-4907-87fd-6c719a6b4586:1 1`，通过这个命令设置一个hash数据结构，这行命令执行后，会出现一个类似下面的数据结构：

        ![image-20200714103404519](F:\java书和视频\笔记\images\面经\3.png)

    2. 锁互斥机制

        客户端1加锁之后，如果客户端要来加锁，首先通过第一个if判断表明已经存在锁了；接着第二if判断该锁的标识ID不是客户端2的，就直接通过pttl返回这个锁key的剩余生存时间。此时客户端2会进入一个while循环，不停的尝试加锁（自旋）。

    https://juejin.im/post/5bf3f15851882526a643e207

### 基于Zookeeper实现分布式锁

每个客户端对某个方法加锁时，在Zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。当释放锁的时候，只需要将这个瞬时节点删除即可。同时可以避免服务宕机导致的锁无法释放，而产生的死锁问题。

[ZooKeeper分布式锁的实现原理](https://juejin.im/post/6844903729406148622)

# --RabbitMQ

## 一、与Spring Boot的整合

1. 添加依赖

    ```java
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-amqp</artifactId>
        <version>2.3.0.RELEASE</version>
    </dependency>
    ```

2. 添加配置信息

    ```yml
    Spring:
      rabbitmq:
        addresses: 192.168.1.107
        port: 5672
        username: admin
        password: admin
        virtual-host: my_vhost
    ```

3. 添加配置类

    ![image-20200714200702786](F:\java书和视频\笔记\images\面经\5.png)

4. 消息发送方和接收方

    ```java
    @Component
    public class HelloSender {
        @Autowired
        private AmqpTemplate rabbitTemplate;
        
        public void send() {
            String context = "hello " + new Date();
            System.out.println("Sender: " + context);
            rabbitTemplate.convertAndSend("hello", context);
        }
    }
    ```

    ```java
    @Component
    @RabbitListener(queues = "hello")//订阅的队列
    public class HelloReiceiver {
        
        @RabbitHandler
        public void process(String hello) {
            System.out.println("Receiver:" + hello);
        }
    }
    
    ```

可以在配置类中配置Exchange和路由绑定关系，也可以使用`AmqpAdmin`设置

```java
 @Test
    public void create() {
        //创建excahnge
        amqpAdmin.declareExchange( new DirectExchange("exchange.direct"));
        amqpAdmin.declareExchange( new FanoutExchange("exchange.fanout"));
        amqpAdmin.declareExchange( new TopicExchange("exchange.topic"));
        
        //创建Queue
        amqpAdmin.declareQueue( new Queue("direct.queue", true));
        amqpAdmin.declareQueue( new Queue("fanout.queue", true));
        
        //绑定Queue
        amqpAdmin.declareBinding(new Binding("direct.queue", Binding.DestinationType.QUEUE, "exchange.direct", "direct.queue", null));
        amqpAdmin.declareBinding(new Binding("fanout.queue", Binding.DestinationType.QUEUE, "exchange.direct", "fanout.queue", null));
        amqpAdmin.declareBinding(new Binding("direct.queue", Binding.DestinationType.QUEUE, "exchange.fanout", "", null));
        amqpAdmin.declareBinding(new Binding("fanout.queue", Binding.DestinationType.QUEUE, "exchange.fanout", "", null));
        amqpAdmin.declareBinding(new Binding("direct.queue", Binding.DestinationType.QUEUE, "exchange.topic", "direct.#", null));
        amqpAdmin.declareBinding(new Binding("fanout.queue", Binding.DestinationType.QUEUE, "exchange.topic", "direct.*", null));
    }
```

```java
@Test
public void send2Direct() {
    Map<String, Object> map = new HashMap<>();
    map.put("msg", "这是一条点对点消息");
    map.put("data", Arrays.asList("hello", 123, true));
    rabbitTemplate.convertAndSend("exchange.direct", "direct.queue", map);
}

@Test
public void receive() {
    Object o = rabbitTemplate.receive("direct.queue");
    Class<?> aClass = o.getClass();
    System.out.println(aClass.getName());
    System.out.println(o);
}
```

**自动配置**

RabbitAutoConfiguration帮我们配置了很多东西：

1. 自动连接工厂：`ConnectionFactory`
2. RabbitProperties：封装了RabbitMQ的配置
3. RabbitTemplate：给RabbitMQ发送和接收消息
4. AmqpAdmin： RabbitMQ系统管理功能组件，用来创建Queue，Exchange，Binding
5. @EnableRabbit+@RabbitListener：监听消息

## 二、消息队列的作用

1. 解耦

2. 冗余存储：在某些情况下处理数据的过程会失败，消息中间件允许把数据持久化直到它们完全被处理

3. 削峰：使用消息中间件可以减少突发访问量增大的压力，不会因为超时负荷而崩溃

    流量削峰一般在秒杀活动中应用广泛，秒杀活动中一般会因为流量过大，导致应用挂掉，为了解决这个问题，一般在应用**前端加入消息队列**

    作用：

    1. 可以控制活动人数，超过此一定阈值的订单直接丢弃
    2. 可以缓解短时间的高流量压垮应用（应用程序按照自己的最大处理能力获取订单）

    用户的请求，服务器收到过后，首先写入消息队列，如果消息队列长度超过超过最大值，则直接抛弃用户请求或跳转到错误页面

    秒杀业务根据消息队列中的请求消息，再做后续处理

## 三、RabbitMQ消息确认机制-可靠抵达

- 发送端

    confirm callback确认模式和return callback 未投递到queue退回模式

    只要消息被broker接收到就会执行confirm callback。被broker接收到消息只能表示message已经到达服务器，并不能保证消息一定会被投递到目标queue里。所以还会用到return callback

* 接收端

    ack机制

## 四、RabbitMQ如何保证消息顺序到达

只设定一个Queue，对应一个consumer，这个consumer内部用内存队列做排队，然后分发给底层不同的worker来处理

![image-20200714222042438](F:\java书和视频\笔记\images\面经\6.png)

# -- Spring

## 一、Bean的生命周期

![image-20200727203409707](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20200727203409707.png)

### setBeanName的作用

设置创建的Bean在容器中的名字

### setBeanFactory的作用

为Bean实例提供其所拥有的factory，将BeanFactory容器实例传入

### setApplicationContext的作用

为Bean实例提供类加载器，将bean所在的应用上下文传进来

### BeanPostProcessor接口的作用

对新构造的实例可以做一些自定义的修改。比如如何构造、属性值的修改、构造器的选择等等。当一个BeanPostProcessor的实现类注册到容器中后，对于该容器中所创建的**每个Bean**实例在初始化方法（如afterPropertiesSet和任意已声明的init方法）调用前，将会调用BeanPostProcessor中的postProcessBeforeInitialization方法，而在bean实例初始化方法调用完成后，则会调用postProcessAfterInitialization方法，整个调用顺序如下:

```xml
--> Spring IOC容器实例化Bean
--> 调用BeanPostProcessor的postProcessBeforeInitialization方法
--> 调用bean实例的初始化方法
--> 调用BeanPostProcessor的postProcessAfterInitialization方法
```

Spring容器通过BeanPostProcessor给我们一个机会对Spring管理的bean进行再加工。比如：可以修改bean的属性，可以**给bean生成一个动态代理**等等。**一些Spring AOP的底层处理也是通过实现BeanPostProcessor来执行代理包装逻辑的。**

## 二、Spring事务中的事务传播行为

### 1、什么是事务传播行为

事务传播行为用来描述由某一个事务传播行为修饰的方法被嵌套进入另一个方法的时候事务如何传播

```java
public void methodA(){
    methodB();
    //doSomething
 }
 
 @Transaction(Propagation=XXX)
 public void methodB(){
    //doSomething
 }
```

代码中`methodA()`方法嵌套调用了`methodB()`方法，`methodB()`的事务传播行为由`@Transaction(Propagation=XXX)`设置决定。这里需要注意的是`methodA()`并没有开启事务，某一个事务传播行为修饰的方法并不是必须要在开启事务的外围方法中调用。

### 2、Spring中的7种事务传播行为

| 事务传播行为类型          | 说明                                                         |
| ------------------------- | ------------------------------------------------------------ |
| PROPAGATION_REQUIRED      | 如果当前没有事务，就新建一个事务；如果已经存在于一个事务当中，加入到该事务中。这是最常见的选择 |
| PROPAGATION_SUPPORTS      | 支持当前事务，如果当前没有事务，就以非事务的方式执行         |
| PROPAGATION_MANDATORY     | 使用当前事务，如果当前没有事务，就抛出异常                   |
| PROPAGATION_REQUIRED_NEW  | 新建事务，如果当前存在事务，就把当前事务挂起                 |
| PROPAGATION_NOT_SUPPORTED | 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起     |
| PROPAGATION_NEVER         | 以非事务方式执行，如果当前存在事务，则抛出异常               |
| PROPAGATION_NESTED        | 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作 |

* PROPAGATION_REQUIRED

    在外围方法**没有开启事务**的情况下，`PROPAGATION_REQUIRED`修饰的内部方法会新开启自己的事务，且**开启的事务相互独立，互不干扰**

    在外围方法**开启事务**的情况下，`PROPAGATION_REQUIRED`修饰的内部方法会加入到外围方法的事务中，所有`PROPAGATION_REQUIRED`修饰的**内部方法和外围方法均属于同一事务，只要一个方法回滚，整个方法都回滚**

* PROPAGATION_REQUIRED_NEW

    在外围方法**没有开启事务**的情况下，`PROPAGATION_REQUIRED_NEW`修饰的内部方法会新开启自己的事务，且**开启的事务相互独立，互补干扰**

    在外围方法**开启事务**的情况下，`PROPAGATION_REQUIRED_NEW`修饰的内部方法依然会**单独开启独立的事务**，且与外部方法的事务也独立，内部方法之间、内部方法和外部方法事务均相互独立，互不干扰

* PROPAGATION_NESTED

    在外围方法开启事务的情况下，`PROPAGATION_NESTED`修饰的内部方法属于外部事务的子事务，外围主事务回滚，子事务一定回滚，而内部子事务可以单独回滚而不影响外围主事务和其他子事务

## 三、AOP

### 1. Spring AOP和AspectJ AOP有什么区别?AOP有哪些实现？

1. AspectJ AOP属于静态代理，也就是AspectJ框架在**编译期间**生成代理类，因此也称为**编译时增强**，运行的时候就是增强之后的AOP对象
2. Spring AOP使用**动态代理**，就是说AOP框架不去修改字节码，而是每次运行时在内存中**临时为方法生成一个AOP对象**（Bean生命周期中BeanPostProcessor接口实现），这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法

AOP的实现就分为静态代理和动态代理

### ==2. JDK动态代理和CgLib动态代理的区别==

Spring中AOP的实现有两种方式，JDK（Java Development Kits）动态代理和CGLib（Byte Code Generation Library）动态代理

- JDK动态代理只提供对接口的代理，不提供类的代理。==主要涉及`java.lang.reflect.Proxy`和`java.lang.reflect.InvocationHandler`两个类==。`InvocationHandler`通过invoke方法反射来调用目标类的代码，动态地将横切逻辑和业务编织在一起；接着Proxy利用InvocationHandler动态创建一个符合某一个接口的实例，生成目标类的代理对象。

    ```java
    public class LogHandler implements InvocationHandler {
        //被代理的对象，实际的方法执行者
        Object target;
        
        public LogHandler(Object target) {
            this.target = target;
        }
        @Override
        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
            
            before();
            Object result = method.invoke(target, args);//调用target的method方法
            after();
            return result;//返回方法的执行结果
        }
        
        public void before() {
            System.out.println(String.format("log start time [%s]", new Date()));
        }
        
        public void after() {
            System.out.println(String.format("log end time [%s]", new Date()));
        }
    }
    
    ```

    ```java
    public class Client {
    
        public static void main(String[] args) {
            //创建被代理类，UserService接口的实现类
            UserServiceImpl userService = new UserServiceImpl();
            //获取对应的ClassLoader
            ClassLoader classLoader = userService.getClass().getClassLoader();
            //获取所有接口的Class	
            Class<?>[] interfaces = userService.getClass().getInterfaces();
            //创建一个将传给代理类的调用请求处理器，处理所有代理对象上的方法调用
            InvocationHandler logHandler = new LogHandler(userService);
            //根据上面提供的信息，创建代理对象，在这个过程中：
            //  a.JDK会通过根据传入的参数信息动态地在内存中创建和.class 文件等同的字节码
            //  b.然后根据相应的字节码转换成对应的class
            //   c.然后调用newInstance()创建代理实例
            UserService proxyInstance = (UserService) Proxy.newProxyInstance(classLoader, interfaces, logHandler);
            
            proxyInstance.select();
            proxyInstance.update();
        }  
    }
    ```

- CGLib动态代理

    如果我们需要被代理的类没有实现任何接口，那么Spring AOP会选择使用CGLib来进行动态代理。CGLib可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并增加增强代码，从而实现AOP。CGLib是通过继承来实现动态代理的，因此如果某个类被标记为final，那么它是无法使用CGLib做动态代理的

    ```java
    
    /**
     * 生产者
     */
    public class Producer{
        
        
        /**
         * 销售
         * @param money
         */
        public void saleProduct(float money) {
            System.out.println("拿到线，销售产品：" + money);
        }
    
        /**
         * 售后
         * @param money
         */
        public void afterService(float money) {
            System.out.println("提供售后服务，并拿到钱：" + money);
        }
    }
    
    ```

    ```java
    /**
     * 模拟一个消费者
     */
    public class Client {
    
        public static void main(String[] args) {
            final Producer producer = new Producer();
    
            /**
             * 动态代理：
             *  特点：字节码随用随创建，随用随加载
             *  作用：不修改源码的基础上对方法增强
             *  分类：
             *      基于接口的动态代理
             *      基于子类的动态代理
             *  基于子类的动态代理
             *      涉及的类：Enhancer
             *      提供者：第三方cglib库
             *  如何创建代理对象：
             *      使用Enhancer类中的create方法
             *  创建代理对象的要求：
             *      被代理类不能是最终类
             *  create方法的参数：
             *      Class：字节码
             *          它是用于指定被代理对象的字节码
             *      Callback：用于提供增强的代码
             *          它是让我们写如何代理，我们一般写的都是该接口的子接口实现类：MethodInterceptor
             */
            Producer proxyProducer = (Producer) Enhancer.create(producer.getClass(), new MethodInterceptor() {
                /**
                 * @param o
                 * @param method
                 * @param objects     
                    以上三个参数和基于接口的动态代理中invoke方法的参数是一样的
                 * @param methodProxy ：当前执行方法的代理对象
                 * @return
                 * @throws Throwable
                 */
                @Override
                public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable {
                    Object returnValue = null;
                    Float money = (Float) objects[0];
                    if ("saleProduct".equals(method.getName())) {
                        returnValue = method.invoke(producer, money*0.8f);
                    }
                    return returnValue;
                }
            });
            proxyProducer.saleProduct(10000f);
        }
    }
    ```

    **JDK动态代理与CGLIB动态代理对比**

    JDK动态代理：基于Java反射机制实现，必须要实现了接口的业务类才能使用这种方法

    CGLib动态代理：基于ASM（字节码操纵框架）机制实现，通过业务生成类的子类作为代理类

    JDK动态代理的优势：

    1. 最小化依赖关系，由于JDK本身的支持，使用更方便
    2. 代码实现简单

    CGLib动态代理的优势：

    1. 无需实现接口
    2. 性能高

# -- SpringMVC

## 一、Spring MVC的主要组件

1. 前端控制器 (DispatcherServlet)

    接收请求，响应结果，相当于转发器，有了DispatcherServlet就减少了其他组件之间的耦合。工作模式相当于DNS中的迭代模式

2. 处理器映射器 (HandlerMapping)

    根据请求的URL来查找相对应的处理器（Handler）

3. 处理器适配器(HandlerAdapter)

    调用具体的处理器

4. 视图解析器

    将处理器返回的消息解析成视图

5. 视图

    View是一个接口，它的实现类支持不同的视图类型（jsp，freemarker等）

## 二、什么是DispatcherServlet

Spring的MVC框架是围绕DispatcherServlet来设计的，它用来**处理所有的HTTP请求和响应**

## 三、SpringMVC的工作流程

<img src="F:\java书和视频\笔记\images\面经\20.png" alt="image-20200729165241634" style="zoom: 200%;" />

1. 用户请求发送至前端控制器DispatcherServlet
2. 前端控制器收到请求后，调用HandlerMapping处理器映射器，请求获取Handler
3. 处理器映射器**根据请求url**找到具体的处理器，生成处理器对象及处理器拦截器（如果有则生成）一并返回给DispatcherServlet
4. DispatcherServlet调用HandlerAdapter处理器适配器
5. HandlerAdapter经过适配调用具体处理器（后端控制器（Controller））
6. Handler执行完成过后返回ModelAndView（包含一个Map属性和一个View）
7. HandlerAdapter将Handler执行结果返回给DispatcherServlet
8. DispatcherServlet将ModelAndView传给ViewResolver进行解析
9. ViewResolver解析后返回具体的View
10. DispatcherServlet对View进行渲染
11. DispatcherServlet响应用户

## 四、@RequestMapping

@RequestMapping注解有6个属性：

1. value：指定请求的实际地址，指定的地址可以是URI Template模式
2. method：指定请求的method类型，GET、POST、PUT、DELETE
3. params：指定请求中必须存在或者不能存在某些参数，还可以指定某个参数必须为某个值
4. headers：指定Http请求中必须包含或者不能包含哪些请求头
5. consumes：指定处理请求的提交内容类型（Content-Type），例如application/json，text/html
6. produces：指定返回的内容类型，仅当request请求头中的Accept类型中包含该指定类型才返回

## 五、ContextLoaderListener的作用

在启动Web容器的时候，读取在contextConfigLocation中定义的xml文件，自动装配ApplicationContext的配置信息，并产生WebApplicationContext对象，然后将这个对象防止在ServletContext的属性里，这样只要得到Servlet就可以得到WebApplicationContext对象，并利用这个对象访问Spring容器管理的Bean



# --MyBatis

## 一、什么是ORM

ORM全称是：Object Relational Mapping（对象关系映射），其主要作用是在编程中，把面向对象的概念跟数据库中的表对应起来，比如创建一个类，就代表数据库中的一张表，类的一个实例就代表表中的一条记录。为了解决关系型数据库数据与Java对象的映射关系的技术。

## 二、MyBatis的优缺点

### 1. 优点

与传统的数据库访问技术相比，ORM有以下优点：

- 基于SQL语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL写在XML里，解除sql与程序代码的耦合，便于统一管理；提供XML标签，支持编写动态SQL语句，并可重用
- 与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接
- 很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持）
- 提供映射标签，支持对象与数据库的ORM字段关系映射；提供对象关系映射标签，支持对象关系组件维护
- 能够与Spring很好的集成

### 2. 缺点

- SQL语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写SQL语句的功底有一定要求
- SQL语句**依赖于数据库，导致数据库移植性差，不能随意更换数据库**

## 三、MyBatis和Hibernate的区别

### 1. 相同点

都是对jdbc的封装，都是持久层的框架，都用于dao层的开发。

### 2. 不同点

映射关系

- MyBatis 是一个半自动映射的框架，配置**Java对象与sql语句执行结果的对应关系**，多表关联关系配置简单
- Hibernate 是一个全表映射的框架，配置**Java对象与数据库表的对应关系**，多表关联关系配置复杂

SQL优化和移植性

- Hibernate 对SQL语句封装，提供了日志、缓存、级联（级联比 MyBatis 强大）等特性，此外还提供 HQL（Hibernate Query Language）操作数据库，数据库无关性支持好，但会多消耗性能。如果项目需要支持多种数据库，代码开发量少，但SQL语句优化困难。
- MyBatis 需要**手动编写 SQL，支持动态 SQL、处理列表、动态生成表名、支持存储过程**。开发**工作量相对大些**。直接使用SQL语句操作数据库，不支持数据库无关性，但sql语句优化容易。

开发难易程度和学习成本

- Hibernate 是重量级框架，学习使用门槛高，适合于需求相对稳定，中小型的项目，比如：办公自动化系统
- MyBatis 是轻量级框架，学习使用门槛低，适合于需求变化频繁，大型的项目，比如：互联网电子商务系统

## 四、MyBatis的工作原理

![image-20200805110449659](F:\java书和视频\笔记\images\面经\35.png)

​	1）读取 MyBatis 配置文件：mybatis-config.xml 为 MyBatis 的全局配置文件，配置了 MyBatis 的运行环境等信息，例如数据库连接信息。

​	2）加载映射文件。映射文件即 SQL 映射文件，该文件中配置了操作数据库的 SQL 语句，需要在 MyBatis 配置文件 mybatis-config.xml 中加载。mybatis-config.xml 文件可以加载多个映射文件，每个文件对应数据库中的一张表。

​	3）构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂 SqlSessionFactory。

​	4）创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。

​	5）Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。

​	6）MappedStatement 对象：在 Executor 接口的执行方法中有一个 MappedStatement 类型的参数，该参数是对映射信息的封装，用于存储要映射的 SQL 语句的 id、参数等信息。

​	7）输入参数映射：输入参数类型可以是 Map、List 等集合类型，也可以是基本数据类型和 POJO 类型。输入参数映射过程类似于 JDBC 对 preparedStatement 对象设置参数的过程。

​	8）输出结果映射：输出结果类型可以是 Map、 List 等集合类型，也可以是基本数据类型和 POJO 类型。输出结果映射过程类似于 JDBC 对结果集的解析过程。

## 五、SQL的预编译？

SQL预编译指的是数据库驱动在发送SQL语句和参数给DBMS之前对SQL语句进行编译，这样DBMS执行SQL时，就不需要重新编译。

- 为什么需要预编译

    JDBC中使用对象PreparedStatement来抽象预编译语句，使用预编译。预编译阶段可以优化SQL的执行。预编译之后的SQL多数情况下可以直接执行，DBMS不需要再次编译，越复杂的SQL，编译的复杂度越大，预编译阶段可以合并多次操作为一个操作。同时预编译语句对象可以重复利用。

    把一个SQL预编译后产生的PreparedStatement对象缓存下来，下次对于同一个SQL，可以直接使用这个缓存的PreparedStatement对象。**MyBatis默认情况下将对所有的SQL进行预编译。**MyBatis底层使用PreparedStatement，过程是先将带有占位符（“？”）的sql模板发送给MySQL服务器，由服务器对此无参数的sql进行编译后，将编译结果缓存，然后直接执行带有真实参数的sql。

    在预编译之前，#{}解析为一个JDBC预编译语句的参数标记符

    ```csharp
    //sqlMap 中如下的 sql 语句
    select * from user where name = #{name};
    //解析成为预编译语句
    select * from user where name = ?;
    ```

    如果${}，SQL解析阶段会进行变量替换，不能实现预编译

    ```csharp
    select * from user where name = '${name}'
    //传递的参数为 "ruhua" 时,解析为如下，然后发送数据库服务器进行编译。
    select * from user where name = "ruhua";
    ```

    **预编译可以优化SQL的执行，还可以防止SQL注入。因为注入进来的参数，系统不会认为这是一条SQL语句，只会认为是一个普通的参数**

    https://www.jianshu.com/p/9972d7b33061

## 六、MyBatis的Executor

MyBatis有三种基本的Executor

### 1. SimpleExecutor

每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象

### 2. ReuseExecutor

执行update或select，以sql作为key查找Statement对象，存在就是用，不存在就创建。使用完过后不关闭，而是存储在Map<String,Statement>内，供下一次使用

### 3. BatchExecutor

执行update（批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行，它缓存了多个Statement对象，每个Statement对象都是addBatch完毕后，等待逐一执行executeBatch批处理

## 七、MyBatis的延迟加载

MyBatis仅支持对association关联对象和collection关联集合对象的懒加载，association指的是一对一，collection指的是一对多查询。在MyBatis配置文件中，可以配置是否启用懒加载：

```xml
aggressiveLazyLoading=false
lazyLoadingEnable=true
```

它的原理是，使用**CGLib创建目标对象的代理对象，当调用目标方法时，进入拦截器方法**，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会**单独发送事先保存好的关联B对象的SQL，把B查询出来**，然后调用a.setB(b)，于是a对象b属性就有值了，接着完成上述方法的调用。这就是延迟加载的原理。

## 八、==#{}和${}的区别==

- #{}是占位符，预编译处理；${}是拼接符（静态文本替换、值替换），字符串替换，没有预编译处理
- MyBatis在处理#{}时，#{}传入参数是以字符串传入，会将SQL中的#{}替换为？，调用PreparedStatement的set方法来赋值。而且变量替换后，#{}对应的变量会自动加上单引号''；而${}对应的变量不会加上单引号

## 九、分页插件的原理+

使用MyBatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，添加对应的物理分页语句和物理分页参数

## 十、MyBatis的插件运行原理，以及如何编写一个插件

MyBatis仅可以针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，MyBatis使用JDK的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4中接口对象的方法时，就会进入拦截方法

# --分布式商城项目

# --区块链溯源项目

